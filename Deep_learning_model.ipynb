{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6qjDawdD3J1"
   },
   "source": [
    "## Notation\n",
    "**Causal identification**\n",
    "\n",
    "- Observed covariates/features: $X$\n",
    "\n",
    "- Potential outcomes: $Y(0)$ and $Y(1)$\n",
    "\n",
    "- Treatment: $T$\n",
    "\n",
    "- Unobservable Individual Treatment Effect: $\\tau_i = Y_i(1) - Y_i(0)$\n",
    "\n",
    "- Average Treatment Effect: $ATE =\\mathbb{E}[Y_i(1)-Y_i(0)]= \\mathbb{E}[{\\tau_i}]$\n",
    "\n",
    "- Conditional Average Treatment Effect: $CATE(x) =\\mathbb{E}[Y_i(1)-Y_i(0)|X=x]$\n",
    "\n",
    "\n",
    "**Deep learning estimation**\n",
    "\n",
    "- Predicted outcomes: $\\hat{Y}(0)$ and $\\hat{Y}(1)$\n",
    "\n",
    "- Outcome modeling functions: $\\hat{Y}(T)=h(X,T)$ \n",
    "\n",
    "- Representation functions: $\\Phi(X)$\n",
    "\n",
    "- Propensity score function:\n",
    "$\\pi(X,T)=P(T|X)$ </br>*where $\\pi(X,1)=P(T=1|X)$ and $\\pi(X,0)=1-\\pi(X,1)$* \n",
    "\n",
    "- Loss functions: $\\mathcal{L}(true,predicted)$, with the mean squared error abbreviated $MSE$ and binary cross-entropy as $BCE$\n",
    "\n",
    "- Estimated CATE<sup>*</sup>: $\\hat{CATE_i} = \\hat{\\tau}_i = \\hat{Y_i}(1)-\\hat{Y_i}(0) = h(X,1)-h(X,0)$\n",
    "\n",
    "- Estimated ATE: $\\hat{ATE}=\\frac{1}{n}\\sum_{i=1}^n\\hat{CATE_i}$\n",
    "\n",
    "- Nearest-neighbor PEHE:\n",
    "$$PEHE_{nn}=\\frac{1}{N}\\sum_{i=1}^N{(\\underbrace{(1−2t_i)(y_i(t_i)−y_i^{nn}(1-t_i)}_{CATE_{nn}}−\\underbrace{(h(\\Phi(x),1)−h(\\Phi(x),0)))}_{\\hat{CATE}}}^2$$ for nearest neighbor $j$ of each unit $i$ in representation space such that $t_j\\neq t_i$:\n",
    "  $$y_i^{nn}(1-t_i) = \\min_{j\\in (1-T)}||\\Phi(x_i|t_i)-\\Phi(x_j|1-t_i)||_2$$\n",
    "\n",
    "\\* We define $\\hat{\\tau}_i = \\hat{CATE_i}$ because the we lack the covariates to estimate the ITE.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELtcaUmUDh8x"
   },
   "source": [
    "## What are we doing here?\n",
    "\n",
    "These model are designed to estimate the  average treatment effect (ATE) and the conditional average treatment effect(CATE) under a selection on observables identification strategy. The ATE is defined as:\n",
    " \n",
    "$$ATE =\\mathbb{E}[Y_i(1)-Y_i(0)]= \\mathbb{E}[{\\tau_i}]$$\n",
    " \n",
    "where $Y_i(1)$ and $Y_i(0)$ are the potential outcomes had unit $i$ received or not received the treatment, respectively. The CATE is defined as,\n",
    " \n",
    "$$CATE(x) =\\mathbb{E}[Y_i(1)-Y_i(0)|X=x]$$\n",
    "\n",
    "where $X$ is the set of selected, observable covariates, and $x \\in X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHh7vdZd1Vnt"
   },
   "source": [
    "## Training metrics for causal inference\n",
    "\n",
    "Although our ultimate goal is to estimate the $\\hat{CATE}$, the loss function in TARNet only minimizes the factual error to estimate $\\hat{Y}$. This is a reflection of the fundamental problem of causal inference: we only observe one potential outcome for each unit.\n",
    "\n",
    "Within this literature, it is common practice to evaluate model performance on simulations using the Precision Estimation of Heterogeneous  Effects (PEHE) from [Hill, 2011](https://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.08162?casa_token=b8-rfzagECIAAAAA:QeP7C4lKN6nZ7MkDjJHFrEberXopD9M5qPBMeBqbk84mI_8qGxj01ctgt4jdZtORpu9aZvpVRe07PA). PEHE measures the error in estimates of the $CATE$:\n",
    "\n",
    "$$PEHE=\\frac{1}{N}\\sum_{i=1}^N(CATE_i-\\hat{CATE_i})^2$$\n",
    "\n",
    "In order to select hyperparameters in real data, [Johansson et al., 2020](https://arxiv.org/pdf/2001.07426.pdf) propose to use a matching variant of $PEHE$ with the nearest Euclidean neighbor of each unit $i$ from the other treatment assignment group $y_i^{nn}$ as a counterfactual. If we identify the nearest neighbor $j$ of each unit $i$ in representation space such that $t_j\\neq t_i$ as\n",
    "\n",
    "  $$y_i^{nn}(1-t_i) = \\min_{j\\in (1-T)}||\\Phi(x_i|t_i)-\\Phi(x_j|1-t_i)||_2$$\n",
    " then,\n",
    "$$PEHE_{nn}=\\frac{1}{N}\\sum_{i=1}^N{(\\underbrace{(1−2t_i)(y_i(t_i)−y_i^{nn}(1-t_i)}_{CATE_{nn}}−\\underbrace{(h(\\Phi(x),1)−h(\\Phi(x),0)))}_{\\hat{CATE}}}^2$$\n",
    "If we take the square root of the $PEHE_{nn}$ then we get an approximation of the unit-level error.\n",
    "\n",
    "I think the intuition behind $\\sqrt{PEHE_{nn}}$ is solid. If our representation function $\\Phi$ is truly learning to balance the treated and control distributions, $CATE_{nn}$ should coarsely measure it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation learning as a balancing strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A core concept in deep learning is the idea that artificial neural networks have the capacity to project a set of complex features $X$ into a useful vector space. When data are transformed into this space, we call the resulting tensor a **representation** ([Goodfellow, et al. 2016](https://www.deeplearningbook.org/contents/representation.html)) (you might also see the term \"embedding\"). For social scientists most comfortable with linear models, we can think about the parameters in each feed-forward layer of a deep neural network as capturing every possible interaction between the values produced by the previous layer. Tasking the network to minimize error on a relevant downstream task encourages it to adjust these interaction parameters to learn useful representations. We can also think about these representation layers as automatically extracting useful  latent covariates/features.\n",
    "\n",
    "The key intuition in this literature is that we want to train neural networks to learn a representation function $\\Phi(X)$ where the data are deconfounded/balanced in the representation space. In other words, the distributions of the representations $\\Phi(X|T=0)$ and $\\Phi(X|T=1)$ are similar.\n",
    "\n",
    "<figure><img src=https://github.com/Gloriagao0624/Uplift/blob/main/WechatIMG77.jpeg?raw=true width=\"900\"></figure>\n",
    "\n",
    "Note that $\\Phi$ must, in theory, be an invertible function for the  ignorability and overlap assumptions to hold. By invertible we mean that there is an inverse function such that $\\Phi^{-1}(\\Phi(X))=X$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: TARNet\n",
    "To encourage balanced representations, [Shalit et al., 2017](http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf) propose a simple two-headed neural network called Treatment Agnostic Regression Network (TARNet). Each head models a separate outcome. One head learns the function $\\hat{Y}(1)=h(\\Phi(X),1)$, and the other head learns the function $\\hat{Y}(0)=h(\\Phi(X),0)$. Both heads backpropagate their gradients to shared representation layers that learn $\\Phi(X)$. Again, the hope is that these representation layers will learn to balance the data because they are used to predict both outcomes.\n",
    "\n",
    "<figure><img src=https://github.com/Gloriagao0624/Uplift/blob/main/WechatIMG60.jpeg?raw=true width=\"900\">This architecture, originally introduced in <a href=http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf>Shalit et al., 2017</a>, is a T-learner with shared representation layers.</figcaption></figure>\n",
    "\n",
    "\n",
    "Other than this architectural change, this has the same loss as the T-Learner:\n",
    "\n",
    "$$\\mathcal{L}(Y,h(\\Phi(X),T))=MSE(Y,h(\\Phi(X),T))=\\frac{1}{n}\\sum_{i=1}^n [h(\\Phi(x_i),t_i)-y_i(t_i)]^2$$\n",
    "\n",
    " The complete objective for the network is to minimize the parameters of $h$ and $\\Phi$ for all $n$ units in the training sample such that,\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{h,\\Phi}\\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(y_i(t_i),h(\\Phi(x_i),t_i)) + \\lambda \\mathcal{R}(h)\\end{equation}\n",
    "\n",
    "where $\\mathcal{R}(h)$ is a model complexity term (e.g., for $L_2$ regularization) and $\\lambda$ is a hyperparameter chosen by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TUbo0kw1_s7"
   },
   "source": [
    "## Loading the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "9a1oDDq-L-w5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path(\"\")\n",
    "df = pd.read_csv(data_path / \"data.csv\")\n",
    "df.rename(columns={'in_ad_group': 'treatment_flg'}, inplace=True)\n",
    "df.rename(columns={'return_label': 'target'}, inplace=True)\n",
    "df = df.loc[(df['country'] =='eg') | (df['country'] =='iq') | (df['country'] =='sa')]\n",
    "#df = df.loc[(df['sample'] =='random_ua') | (df['sample'] =='control_no_ua')]\n",
    "df = df.drop('sample',axis=1)\n",
    "df = df.drop('country',axis=1)\n",
    "df = df.drop('level0',axis=1)\n",
    "df = df.drop('uninstall_cnt',axis=1)\n",
    "df = df.drop('now_uninstall',axis=1)\n",
    "df.sort_values(by=['days_since_reg'],ascending=True)\n",
    "df = df.drop_duplicates(['vopenid'])\n",
    "df = df.set_index(\"vopenid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 31)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=2) \n",
    "typicalNDict = {1: 10000, 0: 20000}\n",
    "\n",
    "def typicalsamling(group, typicalNDict):\n",
    "    name = group.name\n",
    "    n = typicalNDict[name]\n",
    "    return group.sample(n=n)\n",
    "\n",
    "df = df.groupby('target').apply(typicalsamling, typicalNDict)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['target'][:,].astype('float32') #most GPUs only compute 32-bit floats\n",
    "t=df['treatment_flg'][:,].astype('float32')\n",
    "x = df.drop(columns=['target', 'treatment_flg']).astype('float32')\n",
    "standard_scaler_data = StandardScaler().fit_transform(x)\n",
    "df={'x':standard_scaler_data,'t':t,'y':y,'t':t}\n",
    "df['t']=df['t'].values.reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "df['y']=df['y'].values.reshape(-1,1)\n",
    "xt = np.concatenate([x, df['t']], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsTQablc2-n0"
   },
   "source": [
    "## Adding our metrics to tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LDXVq2QXfRna"
   },
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow==2.8.0\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V1pELmlaISz0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "def pdist2sq(x,y):\n",
    "    x2 = tf.reduce_sum(x ** 2, axis=-1, keepdims=True)\n",
    "    y2 = tf.reduce_sum(y ** 2, axis=-1, keepdims=True)\n",
    "    dist = x2 + tf.transpose(y2, (1, 0)) - 2. * x @ tf.transpose(y, (1, 0))\n",
    "    return dist\n",
    "\n",
    "'''\n",
    "def pdist2sq(A, B):\n",
    "    #helper for PEHEnn\n",
    "    #calculates squared euclidean distance between rows of two matrices  \n",
    "    #https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\n",
    "    # squared norms of each row in A and B\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)    \n",
    "    # na as a row and nb as a column vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    # return pairwise euclidean difference matrix\n",
    "    D = tf.sqrt(tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 0.0))\n",
    "    return D\n",
    "'''\n",
    "\n",
    "#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n",
    "class Base_Metrics(Callback):\n",
    "    def __init__(self,data, verbose=0):   \n",
    "        super(Base_Metrics, self).__init__()\n",
    "        self.data=data #feed the callback the full dataset\n",
    "        self.verbose=verbose\n",
    "\n",
    "        #needed for PEHEnn; Called in self.find_ynn\n",
    "        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n",
    "        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n",
    "        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n",
    "    \n",
    "    def split_pred(self,concat_pred):\n",
    "        #this helps us keep ptrack of things so we don't make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0].reshape(-1, 1)\n",
    "        preds['y1_pred'] = concat_pred[:, 1].reshape(-1, 1)\n",
    "        preds['phi'] = concat_pred[:, 2:]\n",
    "        return preds\n",
    "\n",
    "    def find_ynn(self, Phi):\n",
    "        #helper for PEHEnn\n",
    "        PhiC, PhiT =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(self.data['t']),tf.int32),2) #separate control and treated reps\n",
    "        dists=tf.sqrt(pdist2sq(PhiC,PhiT)) #calculate squared distance then sqrt to get euclidean\n",
    "        yT_nn_idx=tf.gather(self.data['c_idx'],tf.argmin(dists,axis=0),1) #get c_idxs of smallest distances for treated units\n",
    "        yC_nn_idx=tf.gather(self.data['t_idx'],tf.argmin(dists,axis=1),1) #get t_idxs of smallest distances for control units\n",
    "        yT_nn=tf.gather(self.data['y'],yT_nn_idx,1) #now use these to retrieve y values\n",
    "        yC_nn=tf.gather(self.data['y'],yC_nn_idx,1)\n",
    "        y_nn=tf.dynamic_stitch([self.data['t_idx'],self.data['c_idx']],[yT_nn,yC_nn]) #stitch em back up!\n",
    "        return y_nn\n",
    "\n",
    "    def PEHEnn(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        y_nn = self.find_ynn(p['phi']) #now its 3 plus because \n",
    "        cate_nn_err=tf.reduce_mean( tf.square( (1-2*self.data['t']) * (y_nn-self.data['y']) - (p['y1_pred']-p['y0_pred']) ) )\n",
    "        return cate_nn_err\n",
    "\n",
    "    def ATE(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        return p['y1_pred']-p['y0_pred']\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        concat_pred=self.model.predict(self.data['x'])\n",
    "        #Calculate Empirical Metrics        \n",
    "        ate_pred=tf.reduce_mean(self.ATE(concat_pred)); tf.summary.scalar('ate', data=ate_pred, step=epoch)\n",
    "        pehe_nn=self.PEHEnn(concat_pred); tf.summary.scalar('cate_nn_err', data=tf.sqrt(pehe_nn), step=epoch)\n",
    "\n",
    "        out_str=f'— cate_nn_err: {tf.sqrt(pehe_nn):.4f} '\n",
    "        if self.verbose > 0: print(out_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACeqUKdM6d2f"
   },
   "source": [
    "## Running the Model\n",
    "Now load the model, loss, and fitting boiler plate from last tutorial.\n",
    "We've made three minor changes. First, we return `phi` in `concat_pred`. Second, we add `BaseMetrics` as a callback. Third, we add some code and a callback to save metrics for later viewing in Tensorboard:\n",
    "```\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0USpaQdXzpKm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 22:12:12.988130: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-28 22:12:12.988822: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 10. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "  320/24000 [..............................] - ETA: 1:14 - loss: 32.3034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 22:12:14.213935: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23872/24000 [============================>.] - ETA: 0s - loss: 23.2482— cate_nn_err: 0.6055 \n",
      "24000/24000 [==============================] - 21s 877us/sample - loss: 23.2318 - val_loss: 20.6877\n",
      "Epoch 2/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 19.6712— cate_nn_err: 0.5908 \n",
      "24000/24000 [==============================] - 19s 786us/sample - loss: 19.6667 - val_loss: 19.6233\n",
      "Epoch 3/20\n",
      "23808/24000 [============================>.] - ETA: 0s - loss: 19.2184— cate_nn_err: 0.5876 \n",
      "24000/24000 [==============================] - 19s 782us/sample - loss: 19.2149 - val_loss: 19.4029\n",
      "Epoch 4/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 19.0460— cate_nn_err: 0.5869 \n",
      "24000/24000 [==============================] - 19s 792us/sample - loss: 19.0502 - val_loss: 19.2549\n",
      "Epoch 5/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 18.9288— cate_nn_err: 0.5870 \n",
      "24000/24000 [==============================] - 19s 789us/sample - loss: 18.9203 - val_loss: 19.1469\n",
      "Epoch 6/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.8062— cate_nn_err: 0.5877 \n",
      "24000/24000 [==============================] - 19s 777us/sample - loss: 18.8041 - val_loss: 19.0312\n",
      "Epoch 7/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.6878— cate_nn_err: 0.5866 \n",
      "24000/24000 [==============================] - 19s 807us/sample - loss: 18.6873 - val_loss: 18.9079\n",
      "Epoch 8/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.5842— cate_nn_err: 0.5866 \n",
      "24000/24000 [==============================] - 19s 782us/sample - loss: 18.5796 - val_loss: 18.8355\n",
      "Epoch 9/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.4729— cate_nn_err: 0.5859 \n",
      "24000/24000 [==============================] - 20s 818us/sample - loss: 18.4725 - val_loss: 18.7065\n",
      "Epoch 10/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 18.3778— cate_nn_err: 0.5863 \n",
      "24000/24000 [==============================] - 20s 850us/sample - loss: 18.3823 - val_loss: 18.6163\n",
      "Epoch 11/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 18.2860— cate_nn_err: 0.5860 \n",
      "24000/24000 [==============================] - 19s 793us/sample - loss: 18.2836 - val_loss: 18.5143\n",
      "Epoch 12/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.1836— cate_nn_err: 0.5860 \n",
      "24000/24000 [==============================] - 19s 774us/sample - loss: 18.1887 - val_loss: 18.4474\n",
      "Epoch 13/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.1137— cate_nn_err: 0.5850 \n",
      "24000/24000 [==============================] - 19s 788us/sample - loss: 18.1069 - val_loss: 18.3391\n",
      "Epoch 14/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.0190— cate_nn_err: 0.5856 \n",
      "24000/24000 [==============================] - 19s 780us/sample - loss: 18.0156 - val_loss: 18.2671\n",
      "Epoch 15/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 17.9382— cate_nn_err: 0.5855 \n",
      "24000/24000 [==============================] - 19s 778us/sample - loss: 17.9363 - val_loss: 18.2141\n",
      "Epoch 16/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 17.8547— cate_nn_err: 0.5858 \n",
      "24000/24000 [==============================] - 19s 777us/sample - loss: 17.8521 - val_loss: 18.1233\n",
      "Epoch 17/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 17.7801— cate_nn_err: 0.5859 \n",
      "24000/24000 [==============================] - 19s 789us/sample - loss: 17.7770 - val_loss: 18.0255\n",
      "Epoch 18/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 17.7082— cate_nn_err: 0.5861 \n",
      "24000/24000 [==============================] - 19s 775us/sample - loss: 17.7035 - val_loss: 17.9688\n",
      "Epoch 19/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 17.6167— cate_nn_err: 0.5857 \n",
      "24000/24000 [==============================] - 19s 778us/sample - loss: 17.6161 - val_loss: 17.9342\n",
      "Epoch 20/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 17.5495— cate_nn_err: 0.5858 \n",
      "24000/24000 [==============================] - 19s 807us/sample - loss: 17.5505 - val_loss: 17.8208\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    " \n",
    "def make_tarnet(input_dim, reg_l2):\n",
    "    '''\n",
    "    The first argument is the column dimension of our data.\n",
    "    It needs to be specified because the functional API creates a static computational graph\n",
    "    The second argument is the strength of regularization we'll apply to the output layers\n",
    "    '''\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    " \n",
    "    # REPRESENTATION\n",
    "    #in TF2/Keras it is idiomatic to instantiate a layer and pass its inputs on the same line unless the layer will be reused\n",
    "    #Note that we apply no regularization to the representation layers \n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n",
    " \n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    " \n",
    "    # second layer\n",
    "    y0_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    y1_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    " \n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    " \n",
    "    #a convenience \"layer\" that concatenates arrays as columns in a matrix\n",
    "    #this time we'll return Phi as well to calculate cate_nn_err\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions, phi])\n",
    "    #the declarations above have specified the computational graph of our network, now we instantiate it\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    " \n",
    "    return model\n",
    " \n",
    "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
    "def regression_loss(concat_true, concat_pred):\n",
    "    #computes a standard MSE loss for TARNet\n",
    "    y_true = concat_true[:, 0] #get individual vectors\n",
    "    t_true = concat_true[:, 1]\n",
    " \n",
    "    y0_pred = concat_pred[:, 0]\n",
    "    y1_pred = concat_pred[:, 1]\n",
    " \n",
    "    #Each head outputs a prediction for both potential outcomes\n",
    "    #We use t_true as a switch to only calculate the factual loss\n",
    "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
    "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
    "    #note Shi uses tf.reduce_sum for her losses even though mathematically we should be using the mean\n",
    "    #tf.reduce_mean and tf.reduce_sum should be equivalent, but maybe having larger error gradients makes training easier?\n",
    "    return loss0 + loss1\n",
    " \n",
    "### MAIN CODE ####\n",
    " \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#make model\n",
    "tarnet_model=make_tarnet(29,.01)\n",
    " \n",
    "#val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([df['y'], df['t']], 1) #we'll use both y and t to compute the loss\n",
    " \n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/fit_tarnet/\n",
    "log_dir = \"logs/fit_tarnet/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "adam_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=1e-8, cooldown=0, min_lr=0),\n",
    "        tensorboard_callback,\n",
    "        Base_Metrics(df,verbose)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "# Split data to train and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(df['x'], yt, test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "tarnet_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "                    loss=regression_loss,\n",
    "                    metric=regression_loss)\n",
    " \n",
    "tarnet_model.fit(x=x_train,y=y_train,\n",
    "                callbacks=adam_callbacks,\n",
    "               # validation_split=val_split,\n",
    "                epochs=20,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=verbose)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tp933mT9vCM"
   },
   "source": [
    "## Evaluating the Model Using Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aedaab8b8ce50492\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aedaab8b8ce50492\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit_tarnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQLj657wOKGL"
   },
   "source": [
    "# Part 2: Hyperparameter Tuning for Statistical Estimators\n",
    "\n",
    "Now that we have metrics for model evaluation that are appropriate to causal inference, we can talk about hyperparameter optimization.\n",
    "Here is a list of potentially tunable hyperparameters for TARNet:\n",
    "\n",
    "Regularization hyperparameters:\n",
    " - $\\lambda$ ($L_2$ regularization strength) for outcome layers\n",
    " - Dropout for outcome modeling layers\n",
    " - Batch normalization\n",
    "\n",
    "Architectural hyperparameters:\n",
    "  - Number of representation layers\n",
    "  - Number of neurons in a representation layer\n",
    "  - Number of output layers\n",
    "  - Number of neurons in an output layer\n",
    "  - Neuronal activation function (e.g. ELU, RELu, Sigmoid)\n",
    "\n",
    "Optimization Hyperparameters:\n",
    " - Choice of Optimizer (e.g. SGD, ADAM)\n",
    " - Optimizer Parameters (e.g. Momentum for SGD)\n",
    " - Learning Rate Scheduling Parameters\n",
    " - Early Stopping Paremeters\n",
    " - Batch Size\n",
    "\n",
    "We are now going to do a hyperparameter search using KerasTuner! We'll select hyperparameter settings that minimize the $\\sqrt{PEHE_{nn}}$.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcMr05p4PfRe"
   },
   "source": [
    "## Building a HyperModel using Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZtKv-IZ2Exo"
   },
   "outputs": [],
   "source": [
    "# Install Keras Tuner\n",
    "#!pip install keras-tuner==1.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "66XXAlr69uXW"
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "def make_hypertarnet(hp):\n",
    "    \"\"\"\n",
    "    Neural net predictive model. The dragon has three heads.\n",
    "    :param input_dim:\n",
    "    :param reg:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # hp.Choice takes hyperparam name, list of options, and default\n",
    "    reg_l2=hp.Choice('l2',[.1,.01,.001],default=.01)\n",
    "    input_dim=29\n",
    "    inputs = Input(shape=(input_dim,), name='input')\n",
    "\n",
    "    # representation\n",
    "    rep_units = hp.Choice('rep_units', [50,100,200,400],default=200)\n",
    "    phi = Dense(units=rep_units, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(inputs)\n",
    "    for i in range(hp.Int('rep_layers', 1, 2, default=1)):\n",
    "      #pretty nifty way to dynamically add more layers!\n",
    "      phi = Dense(units=rep_units, activation='elu', kernel_initializer='RandomNormal',name='phi_'+str(i+2))(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    hyp_units = hp.Choice('hyp_units', [20,50,100,200,400],default=100)\n",
    "    y0_hidden = Dense(units=hyp_units, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=hyp_units, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "    for i in range(hp.Int('hyp_layers', 1, 3, default=2)):\n",
    "        y0_hidden = Dense(units=hyp_units, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_'+str(i+2))(y0_hidden)\n",
    "        y1_hidden = Dense(units=hyp_units, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_'+str(i+2))(y1_hidden)\n",
    "    \n",
    "    # OUTPUT\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,phi])\n",
    "    model = Model(inputs=inputs, outputs=concat_pred)\n",
    "    \n",
    "    sgd_lr = 1e-5\n",
    "    momentum = 0.9\n",
    "    \n",
    "    optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True)\n",
    "    \n",
    "    model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                      loss=regression_loss,\n",
    "                 metric=regression_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-nKhxMbQmib"
   },
   "source": [
    "## Tailoring Keras Tuner for our needs\n",
    "\n",
    "Because we wish to tune models using $\\sqrt{PEHE_{nn}}$ instead of the network's own loss function, we can't use the standard Keras Tuner framework. Instead, we subclass Keras Tuner and reimplement the `run_trial` method. Incidentally, this allows us to add other non-model (i.e., optimizer-related) parameters like the batch size and early-stopping patience.\n",
    "\n",
    " This code should look very familiar by now. The only differences is that we now have a `trial_id` for each parameter configuration which we need to use to save the model and Tensorboard logs. We also add an additional callback for saving these hyperparameter configurations in TensorBoard. Lastly the line,\n",
    "\n",
    "`self.oracle.update_trial(trial.trial_id, {'cate_nn_err': cate_nn_err})`\n",
    "\n",
    "reports the $\\sqrt{PEHE_{nn}}$ back to Keras Tuner so that it can compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NjdkfNP2D37c"
   },
   "outputs": [],
   "source": [
    "from keras_tuner.engine import tuner_utils\n",
    "from tensorboard.plugins.hparams import api as hparams_api\n",
    "!rm -rf my_dir\n",
    "\n",
    "class TarNetTuner(kt.Tuner):\n",
    "\n",
    "    def run_trial(self, trial,dataset,*fit_args, **fit_kwargs):\n",
    "        # *args and **kwargs in Python are positional (list) and keyword (dict) arguments\n",
    "        verbose = fit_kwargs['verbose']\n",
    "\n",
    "        log_dir=self.project_dir+'/trial_'+trial.trial_id\n",
    "        hp = trial.hyperparameters\n",
    "\n",
    "        batch_size = hp.Int('batch_size', 128, 256, step=64, default=128)\n",
    "        stopping_patience=hp.Int('batch_size', 5, 15, step=5, default=5)\n",
    "\n",
    "    \n",
    "        hparams = tuner_utils.convert_hyperparams_to_hparams(trial.hyperparameters)\n",
    "        file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "        file_writer.set_as_default()\n",
    "\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        hparams_callback = hparams_api.KerasCallback(\n",
    "                        writer=log_dir,\n",
    "                        hparams=hparams,\n",
    "                        trial_id='trial_'+trial.trial_id) \n",
    "        metrics_callback=Full_Metrics(dataset,verbose=verbose)\n",
    "        callbacks = [\n",
    "              TerminateOnNaN(),\n",
    "              EarlyStopping(monitor='val_loss', patience=stopping_patience, min_delta=0.0001),\n",
    "              ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                                min_delta=0, cooldown=0, min_lr=0),\n",
    "              metrics_callback,\n",
    "              tensorboard_callback,\n",
    "              hparams_callback\n",
    "          ]\n",
    "\n",
    "\n",
    "        model = self.hypermodel.build(hp)\n",
    "        model.fit(x=fit_args[0],y=fit_args[1],\n",
    "                 callbacks=callbacks,\n",
    "                  validation_split=fit_kwargs['validation_split'],\n",
    "                  epochs=fit_kwargs['epochs'],\n",
    "                  batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "        #give the metric to the hyperparameter optimization algorithm\n",
    "        concat_pred=model.predict(df['x'])\n",
    "        pehe_nn=metrics_callback.PEHEnn(concat_pred)\n",
    "        self.oracle.update_trial(trial.trial_id, {'cate_nn_err': tf.sqrt(pehe_nn)})\n",
    "        self.save_model(trial.trial_id, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "id": "P6o1l7lDQz_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Complete [04h 00m 13s]\n",
      "cate_nn_err: 0.5859698057174683\n",
      "\n",
      "Best cate_nn_err So Far: 0.584354817867279\n",
      "Total elapsed time: 04h 21m 17s\n",
      "\n",
      "Search: Running Trial #5\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "l2                |0.001             |0.01              \n",
      "rep_units         |200               |400               \n",
      "rep_layers        |1                 |2                 \n",
      "hyp_units         |400               |400               \n",
      "hyp_layers        |2                 |2                 \n",
      "batch_size        |192               |128               \n",
      "\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 18:27:39.021276: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:499] constant folding failed: Deadline exceeded: constant folding exceeded deadline., time = 925383.438ms.\n",
      "2022-08-27 18:27:39.024253: W tensorflow/core/common_runtime/process_function_library_runtime.cc:675] Ignoring multi-device function optimization failure: Deadline exceeded: meta_optimizer exceeded deadline.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 18:27:39.958240: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\n",
      "2022-08-27 18:27:39.959611: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at iterator_ops.cc:893 : Not found: Resource AnonymousIterator/AnonymousIterator393/N10tensorflow4data16IteratorResourceE does not exist.\n",
      "2022-08-27 18:27:39.959899: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Not found: Resource AnonymousIterator/AnonymousIterator393/N10tensorflow4data16IteratorResourceE does not exist.\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— cate_nn_err: 0.6144 \n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": " Resource AnonymousIterator/AnonymousIterator393/N10tensorflow4data16IteratorResourceE does not exist.\n\t [[node IteratorGetNext (defined at Users/kouka/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_365825]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8z/50r1vhz14b13y5t1r62h6kv00000gn/T/ipykernel_19958/190601335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mhypermodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_hypertarnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8z/50r1vhz14b13y5t1r62h6kv00000gn/T/ipykernel_19958/2834517186.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, dataset, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m                   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation_split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#give the metric to the hyperparameter optimization algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m:  Resource AnonymousIterator/AnonymousIterator393/N10tensorflow4data16IteratorResourceE does not exist.\n\t [[node IteratorGetNext (defined at Users/kouka/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_365825]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "tuner = TarNetTuner(\n",
    "    #the oracle is the hyperoptimization algorithm\n",
    "    oracle=kt.oracles.BayesianOptimization(\n",
    "        objective=kt.Objective('cate_nn_err', 'min'),\n",
    "        max_trials=10,\n",
    "        seed=0    \n",
    "),\n",
    "        directory='my_dir',\n",
    "        project_name='hypertuner',\n",
    "    hypermodel=make_hypertarnet\n",
    "    )\n",
    "tuner.search(df, df['x'],yt, epochs=30,validation_split=.2,verbose=2)\n",
    "\n",
    "best_trial=tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "print(\"BEST TRIAL ID:\",best_trial.trial_id)\n",
    "best_model=tuner.load_model(best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-Ikqa7ZNF18"
   },
   "source": [
    "## Examing Hyperparameters in Tensorboard\n",
    "\n",
    "Once KerasTuner is done, we can boot TensorBoard back up. This time we'll focus on the \"HPARAMS\" tab. In the \"Table View\" you can compare the best trial to others on the metrics we looked at before. The \"Parallel Cordinates View\" and \"Scatter Plot Matrix View\" have more information though.\n",
    "\n",
    "Let's check out the \"Parallel Cordinates View\" Here you can see trends across metrics and hyperparameterizations. To the far right are the metrics we really care about: `ate_pred`,`cate_nn_err`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PR2VABpBdGAc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-80c3c085b156a5f5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-80c3c085b156a5f5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6014;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir my_dir/hypertuner/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: CFRNet\n",
    "Instead of using  semi-parametric corrections based on the propensity score to adjust ATE estimates, [Shalit et al., 2017](https://arxiv.org/abs/1606.03976), [Johansson et al. 2019](https://arxiv.org/abs/1903.03448), and [Johansson et al., 2020](https://arxiv.org/abs/2001.07426) take a different approach to encourage the representation function $\\Phi$ to explicitly learn treatment effects: integral probability metrics. \n",
    "\n",
    "Integral probability metrics (IPMs) are true metrics (symmetric and obey the triangle inequality unlike KL divergence) that measure the distance between two distributions. **The key idea here is that we can add an IPM as a loss in TARNET to more explicitly encourage the representation layers to balance the covariate distribution of the treated group $T$, and the covariate distribution of the control group $C$.** The variant of TARNet with an IPM loss is called Counterfactual Regression Network (CFRNet). \n",
    "In recent years, IPMs have become very popular losses for generative adversarial networks.   \n",
    "\n",
    "Theoretically, this group has developed generalization bounds for the $CATE$ that show that even though the counterfactual loss is unknowable, it can be bounded by the factual loss of $h$ and an IPM between the treated and control distributions. Here we use MMD as our IPM:\n",
    "## Maximum Mean Discrepancy (MMD)\n",
    "\n",
    "The  maximum  mean  discrepancy (MMD)  is  the  normed  distance between the means of two distributions $T$ and $C$,  after a kernel function has transformed them into a high-dimensional reproducing kernel Hibbert Space (RKHS). It relies on the kernel trick to calculate distances in the RKHS. The metric is built on the intuition that there is no function that would have differing expected values for $T$ and $C$ in this high-dimensional space, if $T$ and $C$ are the same distribution. Formally we can define the MMD as\n",
    "\n",
    "$$MMD(T,C) = ||\\mathbb{E}_{X \\sim T}\\phi(X) - \\mathbb{E}_{X \\sim C}\\phi(X)||^2_{\\mathcal{H}}$$\n",
    "\n",
    "where $\\phi$ is associated with a reproducing kernel function $k$ such as the Gaussian radial basis function kernel: $$k(X,X')=\\exp({- \\frac{||X-X'||^2}{2\\sigma^2} }) $$\n",
    "\n",
    "From [Johansson et al., 2020](https://arxiv.org/abs/2001.07426), an unbiased estimator for the square of the MMD from a sample of size $m$ drawn from treated distribution $T$ and a sample of size $n$ drawn from control distribution $C$ is,\n",
    "\n",
    "$$\\hat{MMD}^2_k(T,C):=\\frac{1}{m(m-1)}\\sum_{i=1}^m\\sum_{i=j}^mk(x_i^T,x_j^T)-\\frac{2}{mn}\\sum_{i=1}^m\\sum_{i=j}^nk(x_i^T,x_j^C)+\\frac{1}{n(n-1)}\\sum_{i=1}^n\\sum_{i=j}^nk(x_i^C,x_j^C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecturally CFRNet is identical to TARNet, but the loss function looks like this:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{h,\\Phi,IPM}\\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(h(\\Phi(x_i),t_i),y_i) + \\lambda \\mathcal{R}(h)+\\alpha\\cdot IPM(\\Phi(X,|T=1),\\Phi(X|T=0))\\end{equation}\n",
    "where $\\mathcal{R}(h)$ is a model complexity term and $\\lambda$ and $\\alpha$ are hyperparameters. $IPM(\\Phi(X,|T=1),\\Phi(X|T=0))$ is an IPM distance between the covariate distributions of the treated and control distributions after they are projected into representation space.\n",
    "\n",
    "<figure><img src=https://github.com/Gloriagao0624/Uplift/blob/main/WechatIMG61.jpeg?raw=true width=\"900\"></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom MMD loss\n",
    "\n",
    "We'll begin by writing up MMD estimator described above in a custom loss object. The layer will take $\\Phi$ and $T$ as inputs, and output $\\hat{MMD^2}$. We'll implement the unbiased MMD estimator described above with the Guassian RBF function. Note that I've chosen to calculate losses with `tf.[reduce_mean]` here because there can be exploding gradients in weighted version of CFRNet (described below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist2sq(x,y):\n",
    "    x2 = tf.reduce_sum(x ** 2, axis=-1, keepdims=True)\n",
    "    y2 = tf.reduce_sum(y ** 2, axis=-1, keepdims=True)\n",
    "    dist = x2 + tf.transpose(y2, (1, 0)) - 2. * x @ tf.transpose(y, (1, 0))\n",
    "    return dist\n",
    "\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class CFRNet_Loss(Loss):\n",
    "  #initialize instance attributes\n",
    "  def __init__(self, alpha=1.,sigma=1.):\n",
    "      super().__init__()\n",
    "      self.alpha = alpha # balances regression loss and MMD IPM\n",
    "      self.rbf_sigma=sigma #for gaussian kernel\n",
    "      self.name='cfrnet_loss'\n",
    "      \n",
    "  def split_pred(self,concat_pred):\n",
    "      #generic helper to make sure we dont make mistakes\n",
    "      preds={}\n",
    "      preds['y0_pred'] = concat_pred[:, 0]\n",
    "      preds['y1_pred'] = concat_pred[:, 1]\n",
    "      preds['phi'] = concat_pred[:, 2:]\n",
    "      return preds\n",
    "\n",
    "  def rbf_kernel(self, x, y):\n",
    "    return tf.exp(-pdist2sq(x,y)/tf.square(self.rbf_sigma))\n",
    "\n",
    "  def calc_mmdsq(self, Phi, t):\n",
    "    Phic, Phit =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(t),tf.int32),2)\n",
    "\n",
    "    Kcc = self.rbf_kernel(Phic,Phic)\n",
    "    Kct = self.rbf_kernel(Phic,Phit)\n",
    "    Ktt = self.rbf_kernel(Phit,Phit)\n",
    "\n",
    "    m = tf.cast(tf.shape(Phic)[0],Phi.dtype)\n",
    "    n = tf.cast(tf.shape(Phit)[0],Phi.dtype)\n",
    "\n",
    "    mmd = 1.0/(m*(m-1.0))*(tf.reduce_sum(Kcc))\n",
    "    mmd = mmd + 1.0/(n*(n-1.0))*(tf.reduce_sum(Ktt))\n",
    "    mmd = mmd - 2.0/(m*n)*tf.reduce_sum(Kct)\n",
    "    return mmd * tf.ones_like(t)\n",
    "\n",
    "  def mmdsq_loss(self, concat_true,concat_pred):\n",
    "    t_true = concat_true[:, 1]\n",
    "    p=self.split_pred(concat_pred)\n",
    "    mmdsq_loss = tf.reduce_mean(self.calc_mmdsq(p['phi'],t_true))\n",
    "    return mmdsq_loss\n",
    "\n",
    "  def regression_loss(self,concat_true,concat_pred):\n",
    "      y_true = concat_true[:, 0]\n",
    "      t_true = concat_true[:, 1]\n",
    "      p = self.split_pred(concat_pred)\n",
    "      loss0 = tf.reduce_mean((1. - t_true) * tf.square(y_true - p['y0_pred']))\n",
    "      loss1 = tf.reduce_mean(t_true * tf.square(y_true - p['y1_pred']))\n",
    "      return loss0+loss1\n",
    "\n",
    "  def cfr_loss(self,concat_true,concat_pred):\n",
    "      lossR = self.regression_loss(concat_true,concat_pred)\n",
    "      lossIPM = self.mmdsq_loss(concat_true,concat_pred)\n",
    "      return lossR + self.alpha * lossIPM\n",
    "\n",
    "      #return lossR + self.alpha * lossIPM\n",
    "\n",
    "  #compute loss\n",
    "  def call(self, concat_true, concat_pred):        \n",
    "      return self.cfr_loss(concat_true,concat_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "def make_cfrnet(input_dim, reg_l2):\n",
    "\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    "\n",
    "    # representation\n",
    "    phi = Dense(units=25, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=25, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "\n",
    "    # second layer\n",
    "    y0_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    y1_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    "\n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,phi])\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      " 2944/24000 [==>...........................] - ETA: 11s - loss: 1.4931 - cfrnet_loss: 0.4342 - regression_loss: 0.3432 - mmdsq_loss: 0.0911"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 22:20:05.604655: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23424/24000 [============================>.] - ETA: 0s - loss: 1.4477 - cfrnet_loss: 0.4015 - regression_loss: 0.3127 - mmdsq_loss: 0.0888— cate_nn_err: 0.5951 \n",
      "24000/24000 [==============================] - 6s 239us/sample - loss: 1.4470 - cfrnet_loss: 0.4012 - regression_loss: 0.3124 - mmdsq_loss: 0.0888 - val_loss: 1.4036 - val_cfrnet_loss: 0.3725 - val_regression_loss: 0.2871 - val_mmdsq_loss: 0.0854\n",
      "Epoch 2/20\n",
      "23744/24000 [============================>.] - ETA: 0s - loss: 1.3728 - cfrnet_loss: 0.3558 - regression_loss: 0.2694 - mmdsq_loss: 0.0864— cate_nn_err: 0.5932 \n",
      "24000/24000 [==============================] - 3s 138us/sample - loss: 1.3725 - cfrnet_loss: 0.3556 - regression_loss: 0.2692 - mmdsq_loss: 0.0864 - val_loss: 1.3378 - val_cfrnet_loss: 0.3352 - val_regression_loss: 0.2508 - val_mmdsq_loss: 0.0843\n",
      "Epoch 3/20\n",
      "23104/24000 [===========================>..] - ETA: 0s - loss: 1.3127 - cfrnet_loss: 0.3233 - regression_loss: 0.2372 - mmdsq_loss: 0.0861— cate_nn_err: 0.5919 \n",
      "24000/24000 [==============================] - 3s 135us/sample - loss: 1.3119 - cfrnet_loss: 0.3231 - regression_loss: 0.2369 - mmdsq_loss: 0.0862 - val_loss: 1.2834 - val_cfrnet_loss: 0.3083 - val_regression_loss: 0.2242 - val_mmdsq_loss: 0.0842\n",
      "Epoch 4/20\n",
      "23040/24000 [===========================>..] - ETA: 0s - loss: 1.2619 - cfrnet_loss: 0.2998 - regression_loss: 0.2143 - mmdsq_loss: 0.0855— cate_nn_err: 0.5916 \n",
      "24000/24000 [==============================] - 3s 134us/sample - loss: 1.2614 - cfrnet_loss: 0.2998 - regression_loss: 0.2141 - mmdsq_loss: 0.0857 - val_loss: 1.2383 - val_cfrnet_loss: 0.2903 - val_regression_loss: 0.2064 - val_mmdsq_loss: 0.0839\n",
      "Epoch 5/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 1.2199 - cfrnet_loss: 0.2853 - regression_loss: 0.2002 - mmdsq_loss: 0.0851— cate_nn_err: 0.5939 \n",
      "24000/24000 [==============================] - 3s 135us/sample - loss: 1.2198 - cfrnet_loss: 0.2852 - regression_loss: 0.2001 - mmdsq_loss: 0.0851 - val_loss: 1.2007 - val_cfrnet_loss: 0.2797 - val_regression_loss: 0.1964 - val_mmdsq_loss: 0.0833\n",
      "Epoch 6/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 1.1852 - cfrnet_loss: 0.2775 - regression_loss: 0.1928 - mmdsq_loss: 0.0848— cate_nn_err: 0.5967 \n",
      "24000/24000 [==============================] - 3s 136us/sample - loss: 1.1851 - cfrnet_loss: 0.2775 - regression_loss: 0.1927 - mmdsq_loss: 0.0848 - val_loss: 1.1681 - val_cfrnet_loss: 0.2741 - val_regression_loss: 0.1916 - val_mmdsq_loss: 0.0825\n",
      "Epoch 7/20\n",
      "22912/24000 [===========================>..] - ETA: 0s - loss: 1.1542 - cfrnet_loss: 0.2728 - regression_loss: 0.1892 - mmdsq_loss: 0.0836— cate_nn_err: 0.5984 \n",
      "24000/24000 [==============================] - 3s 139us/sample - loss: 1.1537 - cfrnet_loss: 0.2730 - regression_loss: 0.1894 - mmdsq_loss: 0.0835 - val_loss: 1.1386 - val_cfrnet_loss: 0.2714 - val_regression_loss: 0.1897 - val_mmdsq_loss: 0.0817\n",
      "Epoch 8/20\n",
      "23424/24000 [============================>.] - ETA: 0s - loss: 1.1252 - cfrnet_loss: 0.2708 - regression_loss: 0.1880 - mmdsq_loss: 0.0828— cate_nn_err: 0.6012 \n",
      "24000/24000 [==============================] - 3s 139us/sample - loss: 1.1249 - cfrnet_loss: 0.2708 - regression_loss: 0.1882 - mmdsq_loss: 0.0826 - val_loss: 1.1108 - val_cfrnet_loss: 0.2701 - val_regression_loss: 0.1888 - val_mmdsq_loss: 0.0812\n",
      "Epoch 9/20\n",
      "23296/24000 [============================>.] - ETA: 0s - loss: 1.0984 - cfrnet_loss: 0.2702 - regression_loss: 0.1877 - mmdsq_loss: 0.0825— cate_nn_err: 0.6032 \n",
      "24000/24000 [==============================] - 3s 137us/sample - loss: 1.0981 - cfrnet_loss: 0.2703 - regression_loss: 0.1877 - mmdsq_loss: 0.0826 - val_loss: 1.0843 - val_cfrnet_loss: 0.2695 - val_regression_loss: 0.1885 - val_mmdsq_loss: 0.0809\n",
      "Epoch 10/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 1.0721 - cfrnet_loss: 0.2699 - regression_loss: 0.1874 - mmdsq_loss: 0.0825— cate_nn_err: 0.6038 \n",
      "24000/24000 [==============================] - 3s 135us/sample - loss: 1.0720 - cfrnet_loss: 0.2699 - regression_loss: 0.1874 - mmdsq_loss: 0.0825 - val_loss: 1.0585 - val_cfrnet_loss: 0.2692 - val_regression_loss: 0.1887 - val_mmdsq_loss: 0.0805\n",
      "Epoch 11/20\n",
      "23296/24000 [============================>.] - ETA: 0s - loss: 1.0467 - cfrnet_loss: 0.2695 - regression_loss: 0.1879 - mmdsq_loss: 0.0816— cate_nn_err: 0.6046 \n",
      "24000/24000 [==============================] - 3s 142us/sample - loss: 1.0462 - cfrnet_loss: 0.2693 - regression_loss: 0.1877 - mmdsq_loss: 0.0816 - val_loss: 1.0335 - val_cfrnet_loss: 0.2691 - val_regression_loss: 0.1886 - val_mmdsq_loss: 0.0805\n",
      "Epoch 12/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 1.0217 - cfrnet_loss: 0.2695 - regression_loss: 0.1877 - mmdsq_loss: 0.0818— cate_nn_err: 0.6050 \n",
      "24000/24000 [==============================] - 3s 140us/sample - loss: 1.0217 - cfrnet_loss: 0.2695 - regression_loss: 0.1878 - mmdsq_loss: 0.0817 - val_loss: 1.0090 - val_cfrnet_loss: 0.2691 - val_regression_loss: 0.1887 - val_mmdsq_loss: 0.0804\n",
      "Epoch 13/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 0.9975 - cfrnet_loss: 0.2694 - regression_loss: 0.1878 - mmdsq_loss: 0.0816— cate_nn_err: 0.6044 \n",
      "24000/24000 [==============================] - 3s 136us/sample - loss: 0.9974 - cfrnet_loss: 0.2693 - regression_loss: 0.1877 - mmdsq_loss: 0.0816 - val_loss: 0.9851 - val_cfrnet_loss: 0.2691 - val_regression_loss: 0.1889 - val_mmdsq_loss: 0.0803\n",
      "Epoch 14/20\n",
      "22976/24000 [===========================>..] - ETA: 0s - loss: 0.9741 - cfrnet_loss: 0.2692 - regression_loss: 0.1875 - mmdsq_loss: 0.0817— cate_nn_err: 0.6041 \n",
      "24000/24000 [==============================] - 3s 136us/sample - loss: 0.9741 - cfrnet_loss: 0.2697 - regression_loss: 0.1879 - mmdsq_loss: 0.0818 - val_loss: 0.9618 - val_cfrnet_loss: 0.2692 - val_regression_loss: 0.1890 - val_mmdsq_loss: 0.0802\n",
      "Epoch 15/20\n",
      "22912/24000 [===========================>..] - ETA: 0s - loss: 0.9515 - cfrnet_loss: 0.2698 - regression_loss: 0.1879 - mmdsq_loss: 0.0818— cate_nn_err: 0.6042 \n",
      "24000/24000 [==============================] - 3s 136us/sample - loss: 0.9513 - cfrnet_loss: 0.2701 - regression_loss: 0.1881 - mmdsq_loss: 0.0819 - val_loss: 0.9389 - val_cfrnet_loss: 0.2692 - val_regression_loss: 0.1892 - val_mmdsq_loss: 0.0801\n",
      "Epoch 16/20\n",
      "23808/24000 [============================>.] - ETA: 0s - loss: 0.9282 - cfrnet_loss: 0.2694 - regression_loss: 0.1882 - mmdsq_loss: 0.0812— cate_nn_err: 0.6051 \n",
      "24000/24000 [==============================] - 5s 229us/sample - loss: 0.9280 - cfrnet_loss: 0.2694 - regression_loss: 0.1882 - mmdsq_loss: 0.0812 - val_loss: 0.9166 - val_cfrnet_loss: 0.2693 - val_regression_loss: 0.1893 - val_mmdsq_loss: 0.0800\n",
      "Epoch 17/20\n",
      "23808/24000 [============================>.] - ETA: 0s - loss: 0.9067 - cfrnet_loss: 0.2701 - regression_loss: 0.1886 - mmdsq_loss: 0.0815— cate_nn_err: 0.6058 \n",
      "24000/24000 [==============================] - 5s 222us/sample - loss: 0.9065 - cfrnet_loss: 0.2699 - regression_loss: 0.1884 - mmdsq_loss: 0.0815 - val_loss: 0.8949 - val_cfrnet_loss: 0.2693 - val_regression_loss: 0.1894 - val_mmdsq_loss: 0.0800\n",
      "Epoch 18/20\n",
      "23744/24000 [============================>.] - ETA: 0s - loss: 0.8851 - cfrnet_loss: 0.2700 - regression_loss: 0.1886 - mmdsq_loss: 0.0814— cate_nn_err: 0.6059 \n",
      "24000/24000 [==============================] - 7s 286us/sample - loss: 0.8848 - cfrnet_loss: 0.2698 - regression_loss: 0.1884 - mmdsq_loss: 0.0814 - val_loss: 0.8736 - val_cfrnet_loss: 0.2695 - val_regression_loss: 0.1896 - val_mmdsq_loss: 0.0798\n",
      "Epoch 19/20\n",
      "23552/24000 [============================>.] - ETA: 0s - loss: 0.8636 - cfrnet_loss: 0.2696 - regression_loss: 0.1885 - mmdsq_loss: 0.0811— cate_nn_err: 0.6066 \n",
      "24000/24000 [==============================] - 6s 248us/sample - loss: 0.8638 - cfrnet_loss: 0.2700 - regression_loss: 0.1888 - mmdsq_loss: 0.0812 - val_loss: 0.8528 - val_cfrnet_loss: 0.2695 - val_regression_loss: 0.1898 - val_mmdsq_loss: 0.0797\n",
      "Epoch 20/20\n",
      "23296/24000 [============================>.] - ETA: 0s - loss: 0.8433 - cfrnet_loss: 0.2698 - regression_loss: 0.1888 - mmdsq_loss: 0.0810— cate_nn_err: 0.6065 \n",
      "24000/24000 [==============================] - 5s 188us/sample - loss: 0.8432 - cfrnet_loss: 0.2700 - regression_loss: 0.1889 - mmdsq_loss: 0.0811 - val_loss: 0.8325 - val_cfrnet_loss: 0.2696 - val_regression_loss: 0.1900 - val_mmdsq_loss: 0.0797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fce64031690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "%load_ext tensorboard \n",
    "\n",
    "batch_size=64\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([df['y'], df['t']], 1)\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/fit_cfrnet/\n",
    "log_dir = \"logs/fit_cfrnet/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split data to train and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(df['x'], yt, test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "adam_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=1e-8, cooldown=0, min_lr=0),\n",
    "        tensorboard_callback,\n",
    "        Base_Metrics(df,verbose=verbose)\n",
    "    ]\n",
    "\n",
    "\n",
    "cfrnet_model=make_cfrnet(df['x'].shape[1],.01)\n",
    "cfrnet_loss=CFRNet_Loss(alpha=1.0)\n",
    "\n",
    "cfrnet_model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "                      loss=cfrnet_loss,\n",
    "                 metrics=[cfrnet_loss,cfrnet_loss.regression_loss,cfrnet_loss.mmdsq_loss])\n",
    "\n",
    "cfrnet_model.fit(x=x_train,y=y_train,\n",
    "                 callbacks=adam_callbacks,\n",
    "                  epochs=20,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=(x_val, y_val),\n",
    "                  verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing results in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fe18d2f2caac52d8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fe18d2f2caac52d8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit_cfrnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding weights to CFRNet\n",
    "A limitation of the original CFRNet is that it does not provide consistency guarantees. In [Johansson et al. 2019](https://arxiv.org/abs/1903.03448), and [Johansson et al., 2020](https://arxiv.org/abs/2001.07426), the authors introduce estimated weights $\\pi(\\Phi(X),T)$ derived from the propensity score to provide consistency guarantees. Interestingly, they also use these weights to relax some of the overlap assumptions as long as the weights themselves obey the positivity assumption. These weights are used to adjust both the representations when calculating the IPM and predicted outcomes.\n",
    "<figure><img src=https://github.com/Gloriagao0624/Uplift/blob/main/weightedcfr.jpeg?raw=true width=\"900\"><figcaption>Weighted CFRNet architecture introduced in Johannson et al., 2020. Purple indicates inputs, orange indicates network layers, other colors indicate output layers, and white indicates outputs. The dashes between colored shapes indicate an unspecifed number of additional hidden layers. The dashed lines on the right indicate non-gradient, plug-in computations that occur after training.</a></figcaption></figure>\n",
    "\n",
    "Weighted CFRNet minimizes the following loss function:\n",
    "\\begin{equation}\n",
    "\\min_{h,\\Phi,IPM,\\pi,\\lambda_h,\\lambda_w}\\frac{1}{n}\\sum_{i=1}^n \\frac{P(t_i)}{\\pi(\\Phi(x_i,t_i))}\\cdot\\mathcal{L}(h(\\Phi(x_i),t_i),y_i) + \\lambda_h \\mathcal{R}(h)+\\alpha\\cdot IPM(\\frac{P(1)}{\\pi(\\Phi(X,1))}\\cdot\\Phi(X,|T=1),\\frac{P(0)}{\\pi(\\Phi(X,0))}\\cdot\\Phi(X|T=0))+\\lambda_\\pi\\frac{||\\pi||_2}{n}\\end{equation}\n",
    "where $R(h)$ is a model complexity term and $\\lambda_h$, $\\lambda_\\pi$ and $\\alpha$ are hyperparameters.\n",
    "\n",
    "Applying the weights during the calculation of the IPM should smooth out some of the noisiness we saw from computing the IPM in tiny batches. This should reduce bias, even if weighting presents a potential variance tradeoff. The last term $\\lambda_\\pi\\frac{||\\pi||_2}{n}$ regularizes the weights to reduce variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weighted_cfrnet(input_dim, reg_l2, weights_l2):\n",
    "\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    "\n",
    "    # representation\n",
    "    phi = Dense(units=32, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=16, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=16, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=16, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "    t_hidden = Dense(units=32, activation='elu', kernel_regularizer=regularizers.l2(weights_l2),name='t_hidden_1')(phi)\n",
    "\n",
    "    # second layer\n",
    "    #y0_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    #y1_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    "    t_hidden = Dense(units=16, activation='elu', kernel_regularizer=regularizers.l2(weights_l2),name='t_hidden_2')(t_hidden)\n",
    "\n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "    t_predictions = Dense(units=1, activation='sigmoid', kernel_regularizer=regularizers.l2(weights_l2), name='t_predictions')(t_hidden)\n",
    "\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,t_predictions,phi])\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class Weighted_CFRNet_Loss(CFRNet_Loss):\n",
    "    #initialize instance attributes\n",
    "    def __init__(self, prob_treat,alpha=1.0,sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.pT=prob_treat\n",
    "        self.alpha = alpha\n",
    "        self.rbf_sigma=sigma\n",
    "        self.name='weighted_cfrnet_loss'\n",
    "    \n",
    "    def split_pred(self,concat_pred):\n",
    "        #generic helper to make sure we dont make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0]\n",
    "        preds['y1_pred'] = concat_pred[:, 1]\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['t_pred'] = (preds['t_pred'] + 0.001) / 1.002\n",
    "        preds['phi'] = concat_pred[:, 3:]\n",
    "        return preds\n",
    "    \n",
    "    #for logging purposes only\n",
    "    def treatment_acc(self,concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        #Since this isn't used as a loss, I've used tf.reduce_mean for interpretability\n",
    "        return tf.reduce_mean(binary_accuracy(t_true, p['t_pred'], threshold=0.5))\n",
    "\n",
    "    def calc_weighted_mmdsq(self, Phi, t_true, t_pred):\n",
    "        t_predC, t_predT  =tf.dynamic_partition(t_pred,tf.cast(tf.squeeze(t_true),tf.int32),2) #propensity\n",
    "        PhiC, PhiT =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(t_true),tf.int32),2) #representation\n",
    "        weightC=tf.expand_dims((1-self.pT)/(1-t_predC),axis=-1)\n",
    "        weightT=tf.expand_dims(self.pT/t_predT,axis=-1)\n",
    "\n",
    "        wPhiC = weightC * PhiC\n",
    "        wPhiT = weightT * PhiT\n",
    "\n",
    " \n",
    "        Kcc = self.rbf_kernel(wPhiC,wPhiC)\n",
    "        Kct = self.rbf_kernel(wPhiC,wPhiT)\n",
    "        Ktt = self.rbf_kernel(wPhiT,wPhiT)\n",
    "\n",
    "        m = tf.cast(tf.shape(PhiC)[0],Phi.dtype)\n",
    "        n = tf.cast(tf.shape(PhiT)[0],Phi.dtype)\n",
    "\n",
    "        mmd = 1.0/(m*(m-1.0))*(tf.reduce_sum(Kcc)-m)\n",
    "        mmd = mmd + 1.0/(n*(n-1.0))*(tf.reduce_sum(Ktt)-n)\n",
    "        mmd = mmd - 2.0/(m*n)*tf.reduce_sum(Kct)\n",
    "        return mmd * tf.ones_like(t_true)\n",
    "\n",
    "    def weighted_mmdsq_loss(self, concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p=self.split_pred(concat_pred)\n",
    "        mmdsq = tf.reduce_mean(self.calc_weighted_mmdsq(p['phi'],t_true, p['t_pred']))\n",
    "        return mmdsq\n",
    "\n",
    "    def weights(self,concat_true,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        weightT=tf.expand_dims(self.pT/p['t_pred'],axis=-1)\n",
    "        return tf.reduce_mean(weightT)\n",
    "\n",
    "    def weighted_regression_loss(self, concat_true, concat_pred):\n",
    "        y_true = concat_true[:, 0]\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "\n",
    "        weightC=tf.expand_dims((1-self.pT)/(1.-p['t_pred']),axis=-1)\n",
    "        weightT=tf.expand_dims(self.pT/p['t_pred'],axis=-1)\n",
    "\n",
    "        loss0 = tf.reduce_mean((1. - t_true) * tf.square(y_true - p['y0_pred'])*weightC)\n",
    "        loss1 = tf.reduce_mean(t_true * tf.square(y_true - p['y1_pred'])* weightT)\n",
    "        return loss0 + loss1\n",
    "\n",
    "    def call(self, concat_true, concat_pred):\n",
    "        return self.weighted_regression_loss(concat_true,concat_pred) + self.alpha * self.weighted_mmdsq_loss(concat_true,concat_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics callback\n",
    "We just need to adjust our metric callback to account for our propensity score predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_Metrics(Base_Metrics):\n",
    "    def __init__(self,data, verbose=0):   \n",
    "        super().__init__(data,verbose)\n",
    "\n",
    "    def split_pred(self,concat_pred):\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0]\n",
    "        preds['y1_pred'] = concat_pred[:, 1]\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['phi'] = concat_pred[:, 3:]\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of treament: 0.6689666666666667\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      " 2624/24000 [==>...........................] - ETA: 18s - loss: 4.8747 - weighted_cfrnet_loss: 0.4257 - weights: 1.3376 - treatment_acc: 0.6608 - weighted_mmdsq_loss: 0.0145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 22:25:08.244737: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23552/24000 [============================>.] - ETA: 0s - loss: 4.7799 - weighted_cfrnet_loss: 0.3906 - weights: 1.3251 - treatment_acc: 0.6932 - weighted_mmdsq_loss: 0.0154— cate_nn_err: 0.6255 \n",
      "24000/24000 [==============================] - 10s 434us/sample - loss: 4.7783 - weighted_cfrnet_loss: 0.3903 - weights: 1.3248 - treatment_acc: 0.6938 - weighted_mmdsq_loss: 0.0155 - val_loss: 4.6738 - val_weighted_cfrnet_loss: 0.3536 - val_weights: 1.3116 - val_treatment_acc: 0.7045 - val_weighted_mmdsq_loss: 0.0152\n",
      "Epoch 2/20\n",
      "23488/24000 [============================>.] - ETA: 0s - loss: 4.5956 - weighted_cfrnet_loss: 0.3403 - weights: 1.2969 - treatment_acc: 0.6988 - weighted_mmdsq_loss: 0.0159— cate_nn_err: 0.6271 \n",
      "24000/24000 [==============================] - 6s 257us/sample - loss: 4.5932 - weighted_cfrnet_loss: 0.3394 - weights: 1.2967 - treatment_acc: 0.6985 - weighted_mmdsq_loss: 0.0159 - val_loss: 4.5034 - val_weighted_cfrnet_loss: 0.3153 - val_weights: 1.2854 - val_treatment_acc: 0.6728 - val_weighted_mmdsq_loss: 0.0165\n",
      "Epoch 3/20\n",
      "23744/24000 [============================>.] - ETA: 0s - loss: 4.4304 - weighted_cfrnet_loss: 0.3062 - weights: 1.2722 - treatment_acc: 0.6764 - weighted_mmdsq_loss: 0.0173— cate_nn_err: 0.6273 \n",
      "24000/24000 [==============================] - 7s 302us/sample - loss: 4.4295 - weighted_cfrnet_loss: 0.3059 - weights: 1.2721 - treatment_acc: 0.6764 - weighted_mmdsq_loss: 0.0174 - val_loss: 4.3481 - val_weighted_cfrnet_loss: 0.2885 - val_weights: 1.2621 - val_treatment_acc: 0.6608 - val_weighted_mmdsq_loss: 0.0179\n",
      "Epoch 4/20\n",
      "23680/24000 [============================>.] - ETA: 0s - loss: 4.2795 - weighted_cfrnet_loss: 0.2818 - weights: 1.2503 - treatment_acc: 0.6677 - weighted_mmdsq_loss: 0.0183— cate_nn_err: 0.6235 \n",
      "24000/24000 [==============================] - 9s 370us/sample - loss: 4.2785 - weighted_cfrnet_loss: 0.2816 - weights: 1.2501 - treatment_acc: 0.6683 - weighted_mmdsq_loss: 0.0183 - val_loss: 4.2029 - val_weighted_cfrnet_loss: 0.2683 - val_weights: 1.2408 - val_treatment_acc: 0.6548 - val_weighted_mmdsq_loss: 0.0187\n",
      "Epoch 5/20\n",
      "23360/24000 [============================>.] - ETA: 0s - loss: 4.1391 - weighted_cfrnet_loss: 0.2640 - weights: 1.2299 - treatment_acc: 0.6672 - weighted_mmdsq_loss: 0.0188— cate_nn_err: 0.6205 \n",
      "24000/24000 [==============================] - 6s 253us/sample - loss: 4.1369 - weighted_cfrnet_loss: 0.2634 - weights: 1.2296 - treatment_acc: 0.6678 - weighted_mmdsq_loss: 0.0189 - val_loss: 4.0653 - val_weighted_cfrnet_loss: 0.2525 - val_weights: 1.2211 - val_treatment_acc: 0.6563 - val_weighted_mmdsq_loss: 0.0187\n",
      "Epoch 6/20\n",
      "23744/24000 [============================>.] - ETA: 0s - loss: 4.0034 - weighted_cfrnet_loss: 0.2495 - weights: 1.2110 - treatment_acc: 0.6689 - weighted_mmdsq_loss: 0.0187— cate_nn_err: 0.6176 \n",
      "24000/24000 [==============================] - 7s 311us/sample - loss: 4.0025 - weighted_cfrnet_loss: 0.2492 - weights: 1.2109 - treatment_acc: 0.6690 - weighted_mmdsq_loss: 0.0187 - val_loss: 3.9344 - val_weighted_cfrnet_loss: 0.2402 - val_weights: 1.2027 - val_treatment_acc: 0.6571 - val_weighted_mmdsq_loss: 0.0182\n",
      "Epoch 7/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 3.8742 - weighted_cfrnet_loss: 0.2380 - weights: 1.1932 - treatment_acc: 0.6697 - weighted_mmdsq_loss: 0.0178— cate_nn_err: 0.6144 \n",
      "24000/24000 [==============================] - 7s 288us/sample - loss: 3.8741 - weighted_cfrnet_loss: 0.2379 - weights: 1.1932 - treatment_acc: 0.6697 - weighted_mmdsq_loss: 0.0178 - val_loss: 3.8091 - val_weighted_cfrnet_loss: 0.2308 - val_weights: 1.1856 - val_treatment_acc: 0.6573 - val_weighted_mmdsq_loss: 0.0173\n",
      "Epoch 8/20\n",
      "23616/24000 [============================>.] - ETA: 0s - loss: 3.7517 - weighted_cfrnet_loss: 0.2291 - weights: 1.1770 - treatment_acc: 0.6706 - weighted_mmdsq_loss: 0.0166— cate_nn_err: 0.6132 \n",
      "24000/24000 [==============================] - 9s 362us/sample - loss: 3.7509 - weighted_cfrnet_loss: 0.2292 - weights: 1.1768 - treatment_acc: 0.6701 - weighted_mmdsq_loss: 0.0166 - val_loss: 3.6888 - val_weighted_cfrnet_loss: 0.2236 - val_weights: 1.1696 - val_treatment_acc: 0.6580 - val_weighted_mmdsq_loss: 0.0164\n",
      "Epoch 9/20\n",
      "23168/24000 [===========================>..] - ETA: 0s - loss: 3.6347 - weighted_cfrnet_loss: 0.2228 - weights: 1.1617 - treatment_acc: 0.6701 - weighted_mmdsq_loss: 0.0156— cate_nn_err: 0.6116 \n",
      "24000/24000 [==============================] - 8s 352us/sample - loss: 3.6327 - weighted_cfrnet_loss: 0.2227 - weights: 1.1615 - treatment_acc: 0.6702 - weighted_mmdsq_loss: 0.0156 - val_loss: 3.5730 - val_weighted_cfrnet_loss: 0.2181 - val_weights: 1.1547 - val_treatment_acc: 0.6580 - val_weighted_mmdsq_loss: 0.0153\n",
      "Epoch 10/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 3.5188 - weighted_cfrnet_loss: 0.2177 - weights: 1.1472 - treatment_acc: 0.6704 - weighted_mmdsq_loss: 0.0147— cate_nn_err: 0.6118 \n",
      "24000/24000 [==============================] - 7s 294us/sample - loss: 3.5186 - weighted_cfrnet_loss: 0.2177 - weights: 1.1472 - treatment_acc: 0.6705 - weighted_mmdsq_loss: 0.0147 - val_loss: 3.4609 - val_weighted_cfrnet_loss: 0.2139 - val_weights: 1.1407 - val_treatment_acc: 0.6581 - val_weighted_mmdsq_loss: 0.0140\n",
      "Epoch 11/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 3.4083 - weighted_cfrnet_loss: 0.2137 - weights: 1.1339 - treatment_acc: 0.6706 - weighted_mmdsq_loss: 0.0131— cate_nn_err: 0.6109 \n",
      "24000/24000 [==============================] - 6s 267us/sample - loss: 3.4080 - weighted_cfrnet_loss: 0.2136 - weights: 1.1339 - treatment_acc: 0.6707 - weighted_mmdsq_loss: 0.0131 - val_loss: 3.3524 - val_weighted_cfrnet_loss: 0.2107 - val_weights: 1.1280 - val_treatment_acc: 0.6583 - val_weighted_mmdsq_loss: 0.0131\n",
      "Epoch 12/20\n",
      "23424/24000 [============================>.] - ETA: 0s - loss: 3.3020 - weighted_cfrnet_loss: 0.2106 - weights: 1.1217 - treatment_acc: 0.6709 - weighted_mmdsq_loss: 0.0122— cate_nn_err: 0.6107 \n",
      "24000/24000 [==============================] - 6s 262us/sample - loss: 3.3007 - weighted_cfrnet_loss: 0.2106 - weights: 1.1215 - treatment_acc: 0.6708 - weighted_mmdsq_loss: 0.0122 - val_loss: 3.2468 - val_weighted_cfrnet_loss: 0.2081 - val_weights: 1.1162 - val_treatment_acc: 0.6583 - val_weighted_mmdsq_loss: 0.0122\n",
      "Epoch 13/20\n",
      "23680/24000 [============================>.] - ETA: 0s - loss: 3.1969 - weighted_cfrnet_loss: 0.2078 - weights: 1.1103 - treatment_acc: 0.6711 - weighted_mmdsq_loss: 0.0114— cate_nn_err: 0.6105 \n",
      "24000/24000 [==============================] - 7s 282us/sample - loss: 3.1962 - weighted_cfrnet_loss: 0.2078 - weights: 1.1103 - treatment_acc: 0.6711 - weighted_mmdsq_loss: 0.0114 - val_loss: 3.1441 - val_weighted_cfrnet_loss: 0.2059 - val_weights: 1.1054 - val_treatment_acc: 0.6585 - val_weighted_mmdsq_loss: 0.0113\n",
      "Epoch 14/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 3.0947 - weighted_cfrnet_loss: 0.2056 - weights: 1.1000 - treatment_acc: 0.6713 - weighted_mmdsq_loss: 0.0105— cate_nn_err: 0.6097 \n",
      "24000/24000 [==============================] - 7s 273us/sample - loss: 3.0945 - weighted_cfrnet_loss: 0.2055 - weights: 1.1000 - treatment_acc: 0.6712 - weighted_mmdsq_loss: 0.0106 - val_loss: 3.0439 - val_weighted_cfrnet_loss: 0.2040 - val_weights: 1.0954 - val_treatment_acc: 0.6585 - val_weighted_mmdsq_loss: 0.0104\n",
      "Epoch 15/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 2.9958 - weighted_cfrnet_loss: 0.2037 - weights: 1.0905 - treatment_acc: 0.6714 - weighted_mmdsq_loss: 0.0098— cate_nn_err: 0.6098 \n",
      "24000/24000 [==============================] - 7s 276us/sample - loss: 2.9955 - weighted_cfrnet_loss: 0.2036 - weights: 1.0905 - treatment_acc: 0.6712 - weighted_mmdsq_loss: 0.0098 - val_loss: 2.9464 - val_weighted_cfrnet_loss: 0.2024 - val_weights: 1.0862 - val_treatment_acc: 0.6585 - val_weighted_mmdsq_loss: 0.0098\n",
      "Epoch 16/20\n",
      "23232/24000 [============================>.] - ETA: 0s - loss: 2.9004 - weighted_cfrnet_loss: 0.2019 - weights: 1.0819 - treatment_acc: 0.6704 - weighted_mmdsq_loss: 0.0091— cate_nn_err: 0.6086 \n",
      "24000/24000 [==============================] - 7s 283us/sample - loss: 2.8988 - weighted_cfrnet_loss: 0.2017 - weights: 1.0817 - treatment_acc: 0.6714 - weighted_mmdsq_loss: 0.0091 - val_loss: 2.8512 - val_weighted_cfrnet_loss: 0.2010 - val_weights: 1.0778 - val_treatment_acc: 0.6583 - val_weighted_mmdsq_loss: 0.0092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "23744/24000 [============================>.] - ETA: 0s - loss: 2.8054 - weighted_cfrnet_loss: 0.2005 - weights: 1.0737 - treatment_acc: 0.6714 - weighted_mmdsq_loss: 0.0086— cate_nn_err: 0.6091 \n",
      "24000/24000 [==============================] - 9s 389us/sample - loss: 2.8046 - weighted_cfrnet_loss: 0.2002 - weights: 1.0737 - treatment_acc: 0.6714 - weighted_mmdsq_loss: 0.0086 - val_loss: 2.7585 - val_weighted_cfrnet_loss: 0.1997 - val_weights: 1.0700 - val_treatment_acc: 0.6583 - val_weighted_mmdsq_loss: 0.0089\n",
      "Epoch 18/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 2.7130 - weighted_cfrnet_loss: 0.1989 - weights: 1.0663 - treatment_acc: 0.6719 - weighted_mmdsq_loss: 0.0082— cate_nn_err: 0.6076 \n",
      "24000/24000 [==============================] - 9s 383us/sample - loss: 2.7128 - weighted_cfrnet_loss: 0.1987 - weights: 1.0662 - treatment_acc: 0.6714 - weighted_mmdsq_loss: 0.0082 - val_loss: 2.6681 - val_weighted_cfrnet_loss: 0.1986 - val_weights: 1.0627 - val_treatment_acc: 0.6583 - val_weighted_mmdsq_loss: 0.0083\n",
      "Epoch 19/20\n",
      "23232/24000 [============================>.] - ETA: 0s - loss: 2.6245 - weighted_cfrnet_loss: 0.1972 - weights: 1.0594 - treatment_acc: 0.6711 - weighted_mmdsq_loss: 0.0078— cate_nn_err: 0.6075 \n",
      "24000/24000 [==============================] - 7s 291us/sample - loss: 2.6234 - weighted_cfrnet_loss: 0.1976 - weights: 1.0593 - treatment_acc: 0.6715 - weighted_mmdsq_loss: 0.0078 - val_loss: 2.5799 - val_weighted_cfrnet_loss: 0.1976 - val_weights: 1.0561 - val_treatment_acc: 0.6583 - val_weighted_mmdsq_loss: 0.0080\n",
      "Epoch 20/20\n",
      "23552/24000 [============================>.] - ETA: 0s - loss: 2.5369 - weighted_cfrnet_loss: 0.1964 - weights: 1.0529 - treatment_acc: 0.6714 - weighted_mmdsq_loss: 0.0075— cate_nn_err: 0.6078 \n",
      "24000/24000 [==============================] - 7s 284us/sample - loss: 2.5362 - weighted_cfrnet_loss: 0.1964 - weights: 1.0529 - treatment_acc: 0.6715 - weighted_mmdsq_loss: 0.0076 - val_loss: 2.4940 - val_weighted_cfrnet_loss: 0.1967 - val_weights: 1.0499 - val_treatment_acc: 0.6583 - val_weighted_mmdsq_loss: 0.0076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fce66a81450>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=64\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([df['y'], df['t']], 1)\n",
    "pT=df['t'][df['t']==1].shape[0]/df['t'].shape[0]\n",
    "print(\"Probability of treament:\", pT)\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/weighted_cfr \n",
    "log_dir = \"logs/weighted_cfr/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "\n",
    "# Split data to train and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(df['x'], yt, test_size=0.2, shuffle=True)\n",
    "\n",
    "adam_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=1e-8, cooldown=0, min_lr=0),\n",
    "        tensorboard_callback,\n",
    "        Weighted_Metrics(df,verbose=verbose)\n",
    "    ]\n",
    "\n",
    "\n",
    "weighted_cfrnet_model=make_weighted_cfrnet(df['x'].shape[1],.001,.1)\n",
    "cfrnet_loss=Weighted_CFRNet_Loss(prob_treat=pT,alpha=1.)\n",
    "\n",
    "weighted_cfrnet_model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "                      loss=cfrnet_loss,\n",
    "                 metrics=[cfrnet_loss,cfrnet_loss.weights,cfrnet_loss.treatment_acc,cfrnet_loss.weighted_mmdsq_loss])\n",
    "\n",
    "weighted_cfrnet_model.fit(x=x_train,y=y_train,\n",
    "                 callbacks=adam_callbacks,\n",
    "                  epochs=20,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=(x_val, y_val),\n",
    "                  verbose=verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing results in Tensorboard\n",
    "\n",
    "The most noticeable thing in our logs is that the `mmdsq_loss` is significantly smoother than in the previous trial. FWIW, the authors also demonstrate in the paper that including the weights lessens the dependence on $\\alpha$ because the weights are learned adaptively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ecc92df6a6479dc8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ecc92df6a6479dc8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/weighted_cfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Drangonnet: semi-parametric extensions to TARNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Beyond outcome modeling, another approach to reducing confounding is adjusting for selection into treatment. This is typically done using the *propensity score*. If the $ATE$ is identifiable by adjusting for $X$, then the propensity score $\\pi(X,T)=P(T|X)$ is sufficient to identify the $ATE$ as well (Rosenbaum and Rubin, 1983). We can estimate the ATE using inverse propensity score weighting:\n",
    " \n",
    "$\\hat{ATE}=[\\frac{T}{\\pi(X,T)}-\\frac{1-T}{\\pi(X,1-T)}]\\cdot Y$\n",
    " \n",
    "To use the IPW estimator with a neural network, we can trivially add a third \"head\" to predict the treatment from the representation $\\Phi$ (actually if we *just* wanted to do IPW we don't need the other two heads at all),\n",
    " \n",
    "$\\hat{ATE}=[\\frac{T}{\\pi(\\Phi(X),T)}-\\frac{1-T}{\\pi(\\Phi(X),1-T)}]\\cdot Y$\n",
    " \n",
    " This is the \"Dragonnet\" architecture from [Shi et al., 2019](https://arxiv.org/pdf/1906.02120.pdf).\n",
    "\n",
    "<figure><img src=https://github.com/Gloriagao0624/Uplift/blob/main/drangonet.jpeg?raw=true width=\"900\"><figcaption>Dragonnet architecture introduced in Shi et al., 2019. This is just TARNet with a third head (single neuron) predicting the propensity score $P(T)=\\pi(\\Phi(X),T)$. Purple represents input data, orange represents representation layers. Red, blue, and green are output layers for control outcome, treated outcome, and propensity score, repsectively. The \"nudge\" parameter is in yellow. Dashes between arrows indicate possible additional hidden layers. Non-gradient, plug-in computation of $\\hat{CATE}$, indicated by dashed lines between white shapes, occurs after training. Figure taken from accompanying review paper.</a></figcaption></figure>\n",
    " \n",
    "The third head could be implemented as a single neuron (as in DragonNet) or using additional layers as in ([Johansson et al. 2018](https://arxiv.org/abs/1903.03448), and [Johansson et al., 2020](https://arxiv.org/abs/2001.07426)) to produce a scalar propensity score $P(T|\\Phi(X))=\\pi(\\Phi(X),T)$.\n",
    " \n",
    "The loss function for this network looks like this:\n",
    "$$\\underset{\\phi,\\pi,h}{\\arg \\min}\\ MSE(Y,h(\\Phi(X),T)) + \\alpha \\cdot \\text{BCE}(T,\\pi(\\Phi(X),T))$$\n",
    "with $\\alpha$ being a hyperparameter to balance the two objectives.\n",
    " \n",
    "Below we break down more sophisticated ways that the propensity score is used in [Shi et al., 2019](https://arxiv.org/pdf/1906.02120.pdf) from semi-parametric estimation theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-parametric theory in three paragraphs\n",
    "\n",
    "The application of semi-parametric theory to causal inference (as far as I understand it), is focused on estimating a target parameter of a distribution $P$ of treatment effects $T(P):=ATE$. While we do not know the true distribution of treatment effects because we lack counterfactuals, we do know some parameters generating this distribution (e.g., the treatment assignment mechanism). We can encode these  constraints in the form of a likelihood that parametrically defines a set of possible approximate distributions of $P$ from our existing data that we'll call $\\mathcal{P}$. Within this set there is a sample-inferred distribution $\\tilde{P}\\in\\mathcal{P}$, that we can use to estimate $T(P)$ using $T(\\tilde{P})$.\n",
    "\n",
    "### Picking $\\tilde{P}$\n",
    "\n",
    "Regardless of $\\tilde{P}$ chosen, $\\tilde{P}\\neq P \\therefore T(\\tilde{P})\\neq T(P)$. We don't really know how to pick $\\tilde{P}$ with finite data to get the best estimate $T(\\tilde{P})$. We can maximize our likelihood function to pick $T(\\tilde{P})$, but there are a lot of \"nuisance\" parameters in the likelihood that are not our target that we don't really care about estimating accurately, so this won't necessarily give us the best estimate of $T(P)$. This is where **influence curves** come in. \n",
    " \n",
    " We're going to define a \"nudge\" parameter $\\epsilon$ that moves $\\tilde{P}$ closer to $P$ (thus moving $T(\\tilde{P})$ closer to $T(P)$). An influence curve of $T(P)$ tells us how changes in $\\epsilon$ will induce changes in $T(P+\\epsilon(\\tilde{P}-P))$. We'll use this influence curve to fit $\\epsilon$ to get the best approximation of $T(P)$ that we can. In particular, there is a specific **efficient influence curve (EIC)** that provides us with the lowest variance (efficient) estimates of $T(P)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIPW\n",
    "\n",
    "The augmented inverse propensity weighting estimator (AIPW or sometimes AIPTW) is an estimator\n",
    "that solves the efficient influence curve estimating equation for the ATE directly (i.e.,  without a nudge parameter). \n",
    "\n",
    "In AIPW (and TMLE), we set the mean of the EIC estimating equation equal to zero which allows us to use it to estimate the $ATE$ linearly. The estimating equation models both the outcome and the treatment. We can specify it as:\n",
    "\n",
    "$EIC = \\frac{1}{N}\\sum_{i=1}^N{[(\\frac{T}{\\pi(\\Phi(X),1)}-\\frac{1-T}{\\pi(\\Phi(X),0)})[Y-h(\\Phi(X),T)] +[h(\\Phi(X),1)-h(\\Phi(X),0)]}]-ATE$\n",
    "\n",
    "$(\\text{Set mean of EIC to 0})$\n",
    "\n",
    "\n",
    "$ATE = \\frac{1}{N}\\sum_{i=1}^N{[\\underbrace{\\underbrace{(\\frac{T}{\\pi(\\Phi(X),1)}-\\frac{1-T}{\\pi(\\Phi(X),0)})}_{\\text{Treatment Modeling}}\\times\\underbrace{[Y-h(\\Phi(X),T)]}_{\\text{Residual Confounding}}}_{\\text{Adjustment}} +\\underbrace{[h(\\Phi(X),1)-h(\\Phi(X),0)]}_{\\text{Outcome Modeling}}}]$\n",
    "\n",
    "There is another interpretation of the AIPW as a \"doubly robust\" estimator. As a doubly robust estimator, we are effectively using Dragonnet to do outcome modeling of $T(\\tilde{P})$ in the second term, but account for any residual confounding (second part of the first term) using a function of the propensity score. Doubly robust estimators are appealing because they will produce a consistent estimate of the $ATE$ if either $\\pi$ or $h$ is estimated consistently, and are efficient if both are estimated correctly to solve the estimating equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow==2.8.0\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.losses import Loss\n",
    "def make_aipw(input_dim, reg_l2):\n",
    "\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    "    # representation\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "\n",
    "    # second layer\n",
    "    y0_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    y1_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    "\n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "\n",
    "    #propensity prediction\n",
    "    #Note that the activation is actually sigmoid, but we will squish it in the loss function for numerical stability reasons\n",
    "    t_prediction = Dense(units=1,activation=None,name='t_prediction')(phi)\n",
    "\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,t_prediction,phi])\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    "    return model\n",
    "\n",
    "class Base_Loss(Loss):\n",
    "    #initialize instance attributes\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.name='standard_loss'\n",
    "\n",
    "    def split_pred(self,concat_pred):\n",
    "        #generic helper to make sure we dont make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0]\n",
    "        preds['y1_pred'] = concat_pred[:, 1]\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['phi'] = concat_pred[:, 3:]\n",
    "        return preds\n",
    "\n",
    "    #for logging purposes only\n",
    "    def treatment_acc(self,concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        #Since this isn't used as a loss, I've used tf.reduce_mean for interpretability\n",
    "        return tf.reduce_mean(binary_accuracy(t_true, tf.math.sigmoid(p['t_pred']), threshold=0.5))\n",
    "\n",
    "    def treatment_bce(self,concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        lossP = tf.reduce_sum(binary_crossentropy(t_true,p['t_pred'],from_logits=True))\n",
    "        return lossP\n",
    "    \n",
    "    def regression_loss(self,concat_true,concat_pred):\n",
    "        y_true = concat_true[:, 0]\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - p['y0_pred']))\n",
    "        loss1 = tf.reduce_sum(t_true * tf.square(y_true - p['y1_pred']))\n",
    "        return loss0+loss1\n",
    "\n",
    "    def standard_loss(self,concat_true,concat_pred):\n",
    "        lossR = self.regression_loss(concat_true,concat_pred)\n",
    "        lossP = self.treatment_bce(concat_true,concat_pred)\n",
    "        return lossR + self.alpha * lossP\n",
    "\n",
    "    #compute loss\n",
    "    def call(self, concat_true, concat_pred):        \n",
    "        return self.standard_loss(concat_true,concat_pred)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add AIPW to our callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "'''\n",
    "def pdist2sq(A, B):\n",
    "    #helper for PEHEnn\n",
    "    #calculates squared euclidean distance between rows of two matrices  \n",
    "    #https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\n",
    "    # squared norms of each row in A and B\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)    \n",
    "    # na as a row and nb as a column vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    # return pairwise euclidean difference matrix\n",
    "    D = tf.sqrt(tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 0.0))\n",
    "    return D\n",
    "'''\n",
    "\n",
    "def pdist2sq(x,y):\n",
    "    x2 = tf.reduce_sum(x ** 2, axis=-1, keepdims=True)\n",
    "    y2 = tf.reduce_sum(y ** 2, axis=-1, keepdims=True)\n",
    "    dist = x2 + tf.transpose(y2, (1, 0)) - 2. * x @ tf.transpose(y, (1, 0))\n",
    "    return dist\n",
    "\n",
    "#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n",
    "class AIPW_Metrics(Callback):\n",
    "    def __init__(self,data, verbose=0):   \n",
    "        super(AIPW_Metrics, self).__init__()\n",
    "        self.data=data #feed the callback the full dataset\n",
    "        self.verbose=verbose\n",
    "\n",
    "        #needed for PEHEnn; Called in self.find_ynn\n",
    "        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n",
    "        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n",
    "        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n",
    "    \n",
    "    def split_pred(self,concat_pred):\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0].reshape(-1, 1)\n",
    "        preds['y1_pred'] = concat_pred[:, 1].reshape(-1, 1)\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['phi'] = concat_pred[:, 3:]\n",
    "        return preds\n",
    "\n",
    "    def find_ynn(self, Phi):\n",
    "        #helper for PEHEnn\n",
    "        PhiC, PhiT =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(self.data['t']),tf.int32),2) #separate control and treated reps\n",
    "        dists=tf.sqrt(pdist2sq(PhiC,PhiT)) #calculate squared distance then sqrt to get euclidean\n",
    "        yT_nn_idx=tf.gather(self.data['c_idx'],tf.argmin(dists,axis=0),1) #get c_idxs of smallest distances for treated units\n",
    "        yC_nn_idx=tf.gather(self.data['t_idx'],tf.argmin(dists,axis=1),1) #get t_idxs of smallest distances for control units\n",
    "        yT_nn=tf.gather(self.data['y'],yT_nn_idx,1) #now use these to retrieve y values\n",
    "        yC_nn=tf.gather(self.data['y'],yC_nn_idx,1)\n",
    "        y_nn=tf.dynamic_stitch([self.data['t_idx'],self.data['c_idx']],[yT_nn,yC_nn]) #stitch em back up!\n",
    "        return y_nn\n",
    "\n",
    "    def PEHEnn(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        y_nn = self.find_ynn(p['phi']) #now its 3 plus because \n",
    "        cate_nn_err=tf.reduce_mean( tf.square( (1-2*self.data['t']) * (y_nn-self.data['y']) - (p['y1_pred']-p['y0_pred']) ) )\n",
    "        return cate_nn_err\n",
    "\n",
    "    def ATE(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        return p['y1_pred']-p['y0_pred']\n",
    "\n",
    "   \n",
    "    #THIS IS THE NEW PART\n",
    "    def AIPW(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        t_pred=tf.math.sigmoid(p['t_pred'])\n",
    "        t_pred = (t_pred + 0.001) / 1.002 # a little numerical stability trick implemented by Shi\n",
    "        y_pred = p['y0_pred'] * (1 - self.data['t']) + p['y1_pred'] * self.data['t']\n",
    "        #cc stands for clever covariate which is I think what it's called in TMLE lit\n",
    "        cc = self.data['t'] * (1.0 / p['t_pred']) - (1.0 - self.data['t']) / (1.0 - p['t_pred'])\n",
    "        cate = cc * (self.data['y'] - y_pred) + p['y1_pred'] - p['y0_pred']\n",
    "        return cate\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        concat_pred=self.model.predict(self.data['x'])\n",
    "        #Calculate Empirical Metrics        \n",
    "        ate_pred=tf.reduce_mean(self.ATE(concat_pred)); tf.summary.scalar('ate', data=ate_pred, step=epoch)\n",
    "        pehe_nn=self.PEHEnn(concat_pred); tf.summary.scalar('cate_nn_err', data=tf.sqrt(pehe_nn), step=epoch)\n",
    "        aipw=tf.reduce_mean(self.AIPW(concat_pred)); tf.summary.scalar('aipw', data=aipw, step=epoch)\n",
    "        out_str=f' — cate_nn_err: {tf.sqrt(pehe_nn):.4f} '\n",
    "        \n",
    "        if self.verbose > 0: print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "  384/24000 [..............................] - ETA: 2:01 - loss: 32.3371 - standard_loss: 24.3027 - regression_loss: 23.6070 - treatment_acc: 0.5130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 23:15:07.127895: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23872/24000 [============================>.] - ETA: 0s - loss: 23.6276 - standard_loss: 15.6235 - regression_loss: 14.9346 - treatment_acc: 0.6090 — cate_nn_err: 0.6042 \n",
      "24000/24000 [==============================] - 41s 2ms/sample - loss: 23.6108 - standard_loss: 15.6069 - regression_loss: 14.9181 - treatment_acc: 0.6092 - val_loss: 21.2507 - val_standard_loss: 13.2721 - val_regression_loss: 12.5899 - val_treatment_acc: 0.6599\n",
      "Epoch 2/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 20.2742 - standard_loss: 12.3392 - regression_loss: 11.6620 - treatment_acc: 0.6753 — cate_nn_err: 0.5891 \n",
      "24000/24000 [==============================] - 33s 1ms/sample - loss: 20.2715 - standard_loss: 12.3366 - regression_loss: 11.6594 - treatment_acc: 0.6753 - val_loss: 20.3128 - val_standard_loss: 12.4078 - val_regression_loss: 11.7358 - val_treatment_acc: 0.6823\n",
      "Epoch 3/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 19.8767 - standard_loss: 12.0149 - regression_loss: 11.3476 - treatment_acc: 0.6964 — cate_nn_err: 0.5879 \n",
      "24000/24000 [==============================] - 33s 1ms/sample - loss: 19.8771 - standard_loss: 12.0155 - regression_loss: 11.3481 - treatment_acc: 0.6961 - val_loss: 20.0673 - val_standard_loss: 12.2381 - val_regression_loss: 11.5753 - val_treatment_acc: 0.6955\n",
      "Epoch 4/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 19.7217 - standard_loss: 11.9347 - regression_loss: 11.2765 - treatment_acc: 0.7110 — cate_nn_err: 0.5879 \n",
      "24000/24000 [==============================] - 48s 2ms/sample - loss: 19.7227 - standard_loss: 11.9358 - regression_loss: 11.2777 - treatment_acc: 0.7111 - val_loss: 19.9138 - val_standard_loss: 12.1597 - val_regression_loss: 11.5054 - val_treatment_acc: 0.7061\n",
      "Epoch 5/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 19.6072 - standard_loss: 11.8955 - regression_loss: 11.2460 - treatment_acc: 0.7245 — cate_nn_err: 0.5866 \n",
      "24000/24000 [==============================] - 36s 1ms/sample - loss: 19.5987 - standard_loss: 11.8872 - regression_loss: 11.2378 - treatment_acc: 0.7248 - val_loss: 19.8081 - val_standard_loss: 12.1292 - val_regression_loss: 11.4830 - val_treatment_acc: 0.7136\n",
      "Epoch 6/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 19.4886 - standard_loss: 11.8529 - regression_loss: 11.2117 - treatment_acc: 0.7308 — cate_nn_err: 0.5888 \n",
      "24000/24000 [==============================] - 36s 1ms/sample - loss: 19.4868 - standard_loss: 11.8511 - regression_loss: 11.2100 - treatment_acc: 0.7308 - val_loss: 19.6958 - val_standard_loss: 12.0931 - val_regression_loss: 11.4547 - val_treatment_acc: 0.7196\n",
      "Epoch 7/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 19.3807 - standard_loss: 11.8205 - regression_loss: 11.1870 - treatment_acc: 0.7345 — cate_nn_err: 0.5873 \n",
      "24000/24000 [==============================] - 33s 1ms/sample - loss: 19.3805 - standard_loss: 11.8203 - regression_loss: 11.1869 - treatment_acc: 0.7345 - val_loss: 19.5866 - val_standard_loss: 12.0589 - val_regression_loss: 11.4276 - val_treatment_acc: 0.7215\n",
      "Epoch 8/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 19.2830 - standard_loss: 11.7979 - regression_loss: 11.1719 - treatment_acc: 0.7359 — cate_nn_err: 0.5870 \n",
      "24000/24000 [==============================] - 35s 1ms/sample - loss: 19.2785 - standard_loss: 11.7935 - regression_loss: 11.1675 - treatment_acc: 0.7358 - val_loss: 19.5389 - val_standard_loss: 12.0869 - val_regression_loss: 11.4627 - val_treatment_acc: 0.7223\n",
      "Epoch 9/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 19.1801 - standard_loss: 11.7697 - regression_loss: 11.1505 - treatment_acc: 0.7366 — cate_nn_err: 0.5856 \n",
      "24000/24000 [==============================] - 34s 1ms/sample - loss: 19.1799 - standard_loss: 11.7696 - regression_loss: 11.1505 - treatment_acc: 0.7366 - val_loss: 19.4006 - val_standard_loss: 12.0230 - val_regression_loss: 11.4050 - val_treatment_acc: 0.7241\n",
      "Epoch 10/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 19.0883 - standard_loss: 11.7519 - regression_loss: 11.1394 - treatment_acc: 0.7376 — cate_nn_err: 0.5863 \n",
      "24000/24000 [==============================] - 36s 1ms/sample - loss: 19.0926 - standard_loss: 11.7564 - regression_loss: 11.1438 - treatment_acc: 0.7374 - val_loss: 19.3208 - val_standard_loss: 12.0162 - val_regression_loss: 11.4037 - val_treatment_acc: 0.7258\n",
      "Epoch 11/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 19.0074 - standard_loss: 11.7444 - regression_loss: 11.1377 - treatment_acc: 0.7386 — cate_nn_err: 0.5860 \n",
      "24000/24000 [==============================] - 36s 1ms/sample - loss: 19.0022 - standard_loss: 11.7393 - regression_loss: 11.1326 - treatment_acc: 0.7385 - val_loss: 19.2200 - val_standard_loss: 11.9884 - val_regression_loss: 11.3815 - val_treatment_acc: 0.7277\n",
      "Epoch 12/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 18.9101 - standard_loss: 11.7193 - regression_loss: 11.1182 - treatment_acc: 0.7397 — cate_nn_err: 0.5860 \n",
      "24000/24000 [==============================] - 34s 1ms/sample - loss: 18.9161 - standard_loss: 11.7256 - regression_loss: 11.1245 - treatment_acc: 0.7395 - val_loss: 19.1681 - val_standard_loss: 12.0086 - val_regression_loss: 11.4069 - val_treatment_acc: 0.7287\n",
      "Epoch 13/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.8474 - standard_loss: 11.7281 - regression_loss: 11.1322 - treatment_acc: 0.7410 — cate_nn_err: 0.5865 \n",
      "24000/24000 [==============================] - 37s 2ms/sample - loss: 18.8405 - standard_loss: 11.7212 - regression_loss: 11.1254 - treatment_acc: 0.7411 - val_loss: 19.0667 - val_standard_loss: 11.9781 - val_regression_loss: 11.3811 - val_treatment_acc: 0.7296\n",
      "Epoch 14/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 18.7568 - standard_loss: 11.7073 - regression_loss: 11.1161 - treatment_acc: 0.7413 — cate_nn_err: 0.5860 \n",
      "24000/24000 [==============================] - 34s 1ms/sample - loss: 18.7543 - standard_loss: 11.7050 - regression_loss: 11.1139 - treatment_acc: 0.7415 - val_loss: 18.9995 - val_standard_loss: 11.9799 - val_regression_loss: 11.3873 - val_treatment_acc: 0.7302\n",
      "Epoch 15/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.6809 - standard_loss: 11.7003 - regression_loss: 11.1138 - treatment_acc: 0.7436 — cate_nn_err: 0.5861 \n",
      "24000/24000 [==============================] - 42s 2ms/sample - loss: 18.6794 - standard_loss: 11.6988 - regression_loss: 11.1122 - treatment_acc: 0.7436 - val_loss: 18.9448 - val_standard_loss: 11.9936 - val_regression_loss: 11.4050 - val_treatment_acc: 0.7306\n",
      "Epoch 16/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.6032 - standard_loss: 11.6900 - regression_loss: 11.1076 - treatment_acc: 0.7448 — cate_nn_err: 0.5857 \n",
      "24000/24000 [==============================] - 38s 2ms/sample - loss: 18.6006 - standard_loss: 11.6875 - regression_loss: 11.1050 - treatment_acc: 0.7446 - val_loss: 18.8656 - val_standard_loss: 11.9812 - val_regression_loss: 11.3963 - val_treatment_acc: 0.7324\n",
      "Epoch 17/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 18.5366 - standard_loss: 11.6894 - regression_loss: 11.1109 - treatment_acc: 0.7459 — cate_nn_err: 0.5853 \n",
      "24000/24000 [==============================] - 39s 2ms/sample - loss: 18.5330 - standard_loss: 11.6861 - regression_loss: 11.1074 - treatment_acc: 0.7458 - val_loss: 18.7813 - val_standard_loss: 11.9621 - val_regression_loss: 11.3805 - val_treatment_acc: 0.7335\n",
      "Epoch 18/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.4696 - standard_loss: 11.6875 - regression_loss: 11.1124 - treatment_acc: 0.7472 — cate_nn_err: 0.5862 \n",
      "24000/24000 [==============================] - 37s 2ms/sample - loss: 18.4651 - standard_loss: 11.6831 - regression_loss: 11.1080 - treatment_acc: 0.7473 - val_loss: 18.7247 - val_standard_loss: 11.9702 - val_regression_loss: 11.3917 - val_treatment_acc: 0.7345\n",
      "Epoch 19/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.3853 - standard_loss: 11.6671 - regression_loss: 11.0950 - treatment_acc: 0.7486 — cate_nn_err: 0.5867 \n",
      "24000/24000 [==============================] - 41s 2ms/sample - loss: 18.3810 - standard_loss: 11.6628 - regression_loss: 11.0909 - treatment_acc: 0.7486 - val_loss: 18.6953 - val_standard_loss: 12.0039 - val_regression_loss: 11.4282 - val_treatment_acc: 0.7347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 18.3186 - standard_loss: 11.6628 - regression_loss: 11.0939 - treatment_acc: 0.7497 — cate_nn_err: 0.5868 \n",
      "24000/24000 [==============================] - 33s 1ms/sample - loss: 18.3196 - standard_loss: 11.6638 - regression_loss: 11.0950 - treatment_acc: 0.7497 - val_loss: 18.5873 - val_standard_loss: 11.9579 - val_regression_loss: 11.3850 - val_treatment_acc: 0.7355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fce5f010d10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "batch_size=64\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([df['y'], df['t']], 1)\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/fit_dragonnet/\n",
    "log_dir = \"logs/fit_dragonnet/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "# Split data to train and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(df['x'], yt, test_size=0.2, shuffle=True)\n",
    "\n",
    "adam_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=1e-8, cooldown=0, min_lr=0),\n",
    "        tensorboard_callback,\n",
    "        AIPW_Metrics(df,verbose=verbose)\n",
    "    ]  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "aipw_model=make_aipw(29,.01)\n",
    "aipw_loss=Base_Loss(alpha=1.0)\n",
    "\n",
    "aipw_model.compile(optimizer=Adam(lr=1e-5),\n",
    "                    loss=aipw_loss,\n",
    "                    metrics=[aipw_loss,aipw_loss.regression_loss,aipw_loss.treatment_acc]\n",
    "                   )\n",
    "\n",
    "aipw_model.fit(x=x_train,y=y_train,\n",
    "                  callbacks=adam_callbacks,\n",
    "                  validation_data=(x_val, y_val),\n",
    "                  epochs=20,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=verbose)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing results in Tensorboard\n",
    "\n",
    "Let's do a quick comparison to last tutorial where we ran the exact same network without the propensity score loss.\n",
    "\n",
    "If we look at `treatment_acc`, it's clear that Dragonnet is learning the treatment information. We can also see that there is only a very slight penalty in the network's ability to predict the outcomes (`ate_err`).\n",
    "\n",
    "It's hard to say whether any other performance differences are significant without doing hyperparameter tuning under both scenarios and looking across multiple simulations. For what it's worth, the `aipw_err` is slightly worse than the raw `ate_err` but it's pretty close, and the statistical guarantees are definitely worth something.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b62d1026e4c33413\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b62d1026e4c33413\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit_dragonnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Targeted Regularization\n",
    " We'll need to start by adding $\\epsilon$ as a parameter in our neural network. Shi does this by creating a [custom layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) she calls `EpsilonLayer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "class EpsilonLayer(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EpsilonLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.epsilon = self.add_weight(name='epsilon',\n",
    "                                       shape=[1, 1],\n",
    "                                       initializer='RandomNormal',\n",
    "                                       #  initializer='ones',\n",
    "                                       trainable=True)\n",
    "        super(EpsilonLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #note there is only one epsilon were just duplicating it for conformability\n",
    "        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n",
    "\n",
    "def make_dragonnet(input_dim, reg_l2):\n",
    "\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    "    # representation\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "\n",
    "    # second layer\n",
    "    y0_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    y1_hidden = Dense(units=200, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    "\n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "\n",
    "    #propensity prediction\n",
    "    #Note that the activation is actually sigmoid, but we will squish it in the loss function for numerical stability reasons\n",
    "    t_predictions = Dense(units=1,activation=None,name='t_prediction')(phi)\n",
    "    #Although the epsilon layer takes an input, it really just houses a free parameter. \n",
    "    epsilons = EpsilonLayer()(t_predictions)\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,t_predictions,epsilons,phi])\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarReg_Loss(Base_Loss):\n",
    "    #initialize instance attributes\n",
    "    def __init__(self, alpha=1,beta=1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta=beta\n",
    "        self.name='tarreg_loss'\n",
    "\n",
    "    def split_pred(self,concat_pred):\n",
    "        #generic helper to make sure we dont make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0]\n",
    "        preds['y1_pred'] = concat_pred[:, 1]\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['epsilon'] = concat_pred[:, 3] #we're moving epsilon into slot three\n",
    "        preds['phi'] = concat_pred[:, 4:]\n",
    "        return preds\n",
    "\n",
    "    def calc_hstar(self,concat_true,concat_pred):\n",
    "        #step 2 above\n",
    "        p=self.split_pred(concat_pred)\n",
    "        y_true = concat_true[:, 0]\n",
    "        t_true = concat_true[:, 1]\n",
    "\n",
    "        t_pred = tf.math.sigmoid(concat_pred[:, 2])\n",
    "        t_pred = (t_pred + 0.001) / 1.002 # a little numerical stability trick implemented by Shi\n",
    "        y_pred = t_true * p['y1_pred'] + (1 - t_true) * p['y0_pred']\n",
    "\n",
    "        #calling it cc for \"clever covariate\" as in SuperLearner TMLE literature\n",
    "        cc = t_true / t_pred - (1 - t_true) / (1 - t_pred)\n",
    "        h_star = y_pred + p['epsilon'] * cc\n",
    "        return h_star\n",
    "\n",
    "    def call(self,concat_true,concat_pred):\n",
    "        y_true = concat_true[:, 0]\n",
    "\n",
    "        standard_loss=self.standard_loss(concat_true,concat_pred)\n",
    "        h_star=self.calc_hstar(concat_true,concat_pred)\n",
    "        #step 3 above\n",
    "        targeted_regularization = tf.reduce_sum(tf.square(y_true - h_star))\n",
    "\n",
    "        # final\n",
    "        loss = standard_loss + self.beta * targeted_regularization\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we update our callback so that it computes $h*$ and the final, plug-in $\\hat{ATE}_{\\text{TR}}$. Looking at the Latex may again be helpful. We can save some code lines by subclassing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarReg_Metrics(AIPW_Metrics):\n",
    "    def __init__(self,data, verbose=0):   \n",
    "        super().__init__(data,verbose)\n",
    "\n",
    "    def split_pred(self,concat_pred):\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0].reshape(-1, 1)\n",
    "        preds['y1_pred'] = concat_pred[:, 1].reshape(-1, 1)\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['epsilon'] = concat_pred[:, 3]\n",
    "        preds['phi'] = concat_pred[:, 4:]\n",
    "        return preds\n",
    "    \n",
    "    def compute_hstar(self,y0_pred,y1_pred,t_pred,t_true,epsilons):\n",
    "        #helper for calculating the targeted regularization cate\n",
    "        y_pred = t_true * y1_pred + (1 - t_true) * y0_pred\n",
    "        cc = t_true / t_pred - (1 - t_true) / (1 - t_pred)\n",
    "        h_star = y_pred + epsilons * cc\n",
    "        return h_star\n",
    "    \n",
    "    def TARREG_CATE(self,concat_pred):\n",
    "        #Final calculation of Targeted Regularization loss\n",
    "        p = self.split_pred(concat_pred)\n",
    "        t_pred = tf.math.sigmoid(p['t_pred'])\n",
    "        t_pred = (t_pred + 0.001) / 1.002 # a little numerical stability trick implemented by Shi       \n",
    "        hstar_0=self.compute_hstar(p['y0_pred'],p['y1_pred'],t_pred,tf.zeros_like(p['epsilon']),p['epsilon'])\n",
    "        hstar_1=self.compute_hstar(p['y0_pred'],p['y1_pred'],t_pred,tf.ones_like(p['epsilon']),p['epsilon'])\n",
    "        return hstar_1-hstar_0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        concat_pred=self.model.predict(self.data['x'])\n",
    "        #Calculate Empirical Metrics        \n",
    "        aipw_pred=tf.reduce_mean(self.AIPW(concat_pred)); tf.summary.scalar('aipw', data=aipw_pred, step=epoch)\n",
    "        ate_pred=tf.reduce_mean(self.ATE(concat_pred)); tf.summary.scalar('ate', data=ate_pred, step=epoch)\n",
    "        tarreg_pred=tf.reduce_mean(self.TARREG_CATE(concat_pred)); tf.summary.scalar('tarreg_pred', data=tarreg_pred, step=epoch)\n",
    "        pehe_nn=self.PEHEnn(concat_pred); tf.summary.scalar('cate_nn_err', data=tf.sqrt(pehe_nn), step=epoch)\n",
    "        \n",
    "        out_str=f'— cate_nn_err: {tf.sqrt(pehe_nn):.4f}'\n",
    "        \n",
    "        if self.verbose > 0: print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "  448/24000 [..............................] - ETA: 1:02 - loss: 65.7172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 23:58:17.899914: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23936/24000 [============================>.] - ETA: 0s - loss: 58.0502— cate_nn_err: 0.6093\n",
      "24000/24000 [==============================] - 63s 3ms/sample - loss: 57.9892 - val_loss: 36.8352\n",
      "Epoch 2/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 35.0835— cate_nn_err: 0.6188\n",
      "24000/24000 [==============================] - 67s 3ms/sample - loss: 35.0790 - val_loss: 33.9772\n",
      "Epoch 3/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 33.2239— cate_nn_err: 0.6286\n",
      "24000/24000 [==============================] - 69s 3ms/sample - loss: 33.2327 - val_loss: 33.4493\n",
      "Epoch 4/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 32.8396— cate_nn_err: 0.6298\n",
      "24000/24000 [==============================] - 68s 3ms/sample - loss: 32.8469 - val_loss: 33.2100\n",
      "Epoch 5/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 32.6571— cate_nn_err: 0.6294\n",
      "24000/24000 [==============================] - 65s 3ms/sample - loss: 32.6399 - val_loss: 33.0752\n",
      "Epoch 6/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 32.4825— cate_nn_err: 0.6313\n",
      "24000/24000 [==============================] - 71s 3ms/sample - loss: 32.4803 - val_loss: 32.9288\n",
      "Epoch 7/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 32.3400— cate_nn_err: 0.6267\n",
      "24000/24000 [==============================] - 66s 3ms/sample - loss: 32.3478 - val_loss: 32.8039\n",
      "Epoch 8/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 32.2329— cate_nn_err: 0.6302\n",
      "24000/24000 [==============================] - 68s 3ms/sample - loss: 32.2247 - val_loss: 32.7543\n",
      "Epoch 9/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 32.1181— cate_nn_err: 0.6309\n",
      "24000/24000 [==============================] - 71s 3ms/sample - loss: 32.1191 - val_loss: 32.5973\n",
      "Epoch 10/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 32.0148— cate_nn_err: 0.6291\n",
      "24000/24000 [==============================] - 65s 3ms/sample - loss: 32.0230 - val_loss: 32.5058\n",
      "Epoch 11/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 31.9383— cate_nn_err: 0.6253\n",
      "24000/24000 [==============================] - 66s 3ms/sample - loss: 31.9322 - val_loss: 32.3906\n",
      "Epoch 12/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 31.8346— cate_nn_err: 0.6247\n",
      "24000/24000 [==============================] - 66s 3ms/sample - loss: 31.8461 - val_loss: 32.3585\n",
      "Epoch 13/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 31.7875— cate_nn_err: 0.6243\n",
      "24000/24000 [==============================] - 65s 3ms/sample - loss: 31.7743 - val_loss: 32.2444\n",
      "Epoch 14/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 31.6928— cate_nn_err: 0.6253\n",
      "24000/24000 [==============================] - 71s 3ms/sample - loss: 31.6892 - val_loss: 32.1865\n",
      "Epoch 15/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 31.6219— cate_nn_err: 0.6232\n",
      "24000/24000 [==============================] - 67s 3ms/sample - loss: 31.6190 - val_loss: 32.1556\n",
      "Epoch 16/20\n",
      "23936/24000 [============================>.] - ETA: 0s - loss: 31.5451— cate_nn_err: 0.6247\n",
      "24000/24000 [==============================] - 69s 3ms/sample - loss: 31.5399 - val_loss: 32.0661\n",
      "Epoch 17/20\n",
      "23872/24000 [============================>.] - ETA: 0s - loss: 31.4864— cate_nn_err: 0.6243\n",
      "24000/24000 [==============================] - 4070s 170ms/sample - loss: 31.4788 - val_loss: 31.9869\n",
      "Epoch 18/20\n",
      "23936/24000 [============================>.] - ETA: 8s - loss: 31.4231 — cate_nn_err: 0.6233\n",
      "24000/24000 [==============================] - 16901s 704ms/sample - loss: 31.4137 - val_loss: 31.9500\n",
      "Epoch 19/20\n",
      "23936/24000 [============================>.] - ETA: 34s - loss: 31.3257 — cate_nn_err: 0.6198\n",
      "24000/24000 [==============================] - 24338s 1s/sample - loss: 31.3176 - val_loss: 31.9601\n",
      "Epoch 20/20\n",
      "23872/24000 [============================>.] - ETA: 14s - loss: 31.2734— cate_nn_err: 0.6205\n",
      "24000/24000 [==============================] - 14457s 602ms/sample - loss: 31.2622 - val_loss: 31.8098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc9b056ff10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "batch_size=64\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([df['y'], df['t']], 1)\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/dragonnet_tarReg/\n",
    "log_dir = \"logs/dragonnet_tarReg/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "# Split data to train and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(df['x'], yt, test_size=0.2, shuffle=True)\n",
    "\n",
    "adam_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=1e-8, cooldown=0, min_lr=0),\n",
    "        tensorboard_callback,\n",
    "        TarReg_Metrics(df,verbose=verbose)\n",
    "    ]  \n",
    "\n",
    "\n",
    "\n",
    "dragonnet_model=make_dragonnet(df['x'].shape[1],.01)\n",
    "tarreg_loss=TarReg_Loss(alpha=1)\n",
    "\n",
    "dragonnet_model.compile(#optimizer=SGD(learning_rate=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                 optimizer=Adam(lr=1e-5),\n",
    "                 loss=tarreg_loss,\n",
    "                 metric=[tarreg_loss,tarreg_loss.regression_loss,tarreg_loss.treatment_acc])\n",
    "\n",
    "dragonnet_model.fit(x=x_train,y=y_train,\n",
    "                 callbacks=adam_callbacks,\n",
    "                 validation_data=(x_val, y_val),\n",
    "                  epochs=20,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing results in Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 39708."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/dragonnet_tarReg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial 2: Causal Inference Metrics and Hyperparameter Optimization.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
